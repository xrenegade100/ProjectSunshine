{"cells":[{"cell_type":"markdown","source":["# Connect"],"metadata":{"id":"WWUcTAG5s3oK"}},{"cell_type":"markdown","source":["## Google Drive\n","\n","The datasets containing the examples of Linguistic Antipattern that the model will have to be able to identify are located in a shared folder, created specifically for the project, on google drive, to facilitate data recovery operations they are then accessed to Drive, so you can then easily retrieve the datasets"],"metadata":{"id":"oIT_NNK1tEXb"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oENJ3x1S-J1x","outputId":"3aa6e99a-b73a-46fe-e965-eb4fe3d9f62a","executionInfo":{"status":"ok","timestamp":1685441860473,"user_tz":-120,"elapsed":23000,"user":{"displayName":"GIOVANNI SCORZIELLO","userId":"08861605982515496649"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Mount in the Colab runtime a folder corresponding to your google drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive');"]},{"cell_type":"markdown","source":["## Hugging Face\n","\n","We log in to Hugging Face so that we can upload the model lastly after finishing the operations necessary for Fine Tuning"],"metadata":{"id":"KVJXxVkltJs4"}},{"cell_type":"code","source":["!pip install huggingface"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UEQE9U_igSt9","executionInfo":{"status":"ok","timestamp":1685430835120,"user_tz":-120,"elapsed":10097,"user":{"displayName":"GIOVANNI SCORZIELLO","userId":"08861605982515496649"}},"outputId":"f5267d48-48d8-44e8-a472-76f00910ccf8"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting huggingface\n","  Downloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\n","Installing collected packages: huggingface\n","Successfully installed huggingface-0.0.1\n"]}]},{"cell_type":"code","source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"],"metadata":{"id":"ID1M3Pr7tLaa","colab":{"base_uri":"https://localhost:8080/","height":359,"referenced_widgets":["99165c7f5d394e34b5cdd938d7b69fec","08a9319cb8674e4cabeaf9d671081584","cd745a885f3d405ab55569caaa6a1aef","87d78009208d4beda9162d1ebc6213a5","e1839a0f3522486185aa4c11d02c29e7","5ae9cdfe17cb4189a713e6a7e5527527","7273c7fbd12a4be396e55372338cdd43","f493abb4ba4f4345956d93db30b1e18b","13a582c194ea49398592be8a8dbcf79f","f528d4dc6c4e43f9880e4b6479202d70","ad058368e5f9432b8bc65114ec82b748","9128c88cc08848db91a17c7a2eef2fe4","fd90784618b442bcb973c0d8193b9316","80992bef008d470dad3291c85c45163d","047dc85b0058420a86b35df924cf086e","f7a023a01cc742e6a41ade21eacf59d1","69c9fa74b3214669943bc565231668e3"]},"executionInfo":{"status":"ok","timestamp":1685430838912,"user_tz":-120,"elapsed":496,"user":{"displayName":"GIOVANNI SCORZIELLO","userId":"08861605982515496649"}},"outputId":"b05a8727-50da-4db3-9ea7-a5002f3ab525"},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99165c7f5d394e34b5cdd938d7b69fec"}},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"vJqFTnXcejOU"},"source":["# Installation"]},{"cell_type":"markdown","source":["## Installing Tranformers Libraries"],"metadata":{"id":"drOV_bDgt5R0"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xlVjB6wjeGRC","outputId":"bdd17ef8-1ae2-4e73-d3ab-f720706f1833","executionInfo":{"status":"ok","timestamp":1685441883694,"user_tz":-120,"elapsed":14361,"user":{"displayName":"GIOVANNI SCORZIELLO","userId":"08861605982515496649"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n"]}],"source":["!pip install transformers"]},{"cell_type":"markdown","metadata":{"id":"Loa3B16qfM7V"},"source":["## Installing the Necessary Libraries\n","\n","In the next step we go instead to download all those libraries necessary to be able to carry out training, evaluation and testing of the model, also adding some libraries used to improve data visualization:\n","\n","1.   **train_test_split**: this function comes from the _sklearn_ library, and allows you to divide a dataset into two parts: _training-set_ and _test-set_. Respectively, one will be used to train the model, while the other will be used to evaluate its generalization ability.\n","2.   **pandas**: offers tools for analyzing data in tabular form, dataframes and manipulating them. It allows operations such as column filtering, aggregation and merging of dataframes.\n","3.  **numpy**: processing of multidimensional numerical arrays, for the latter, offers linear algebra operations and mathematical operations.\n","4.  **tabulate**: allows you to transform an array into a table to graphically display data structures.\n","5.  **tqdm**: allows displaying progressbar in iterative processing loops. It could be useful at this stage of system development, to monitor the time required by the various processes."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"TPsZ223TfYAs","executionInfo":{"status":"ok","timestamp":1685441904328,"user_tz":-120,"elapsed":6946,"user":{"displayName":"GIOVANNI SCORZIELLO","userId":"08861605982515496649"}}},"outputs":[],"source":["import torch # ML framework\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler # library for data manipulation\n","\n","import pandas as pd\n","import numpy as np\n","\n","from tabulate import tabulate\n","from tqdm import trange\n","import random # random numbers generation\n","\n","import warnings\n","warnings.filterwarnings(action='once')"]},{"cell_type":"markdown","source":["# Data Preparation"],"metadata":{"id":"VWHiKrvZXsXa"}},{"cell_type":"markdown","source":["## Retrive Data from Dataset"],"metadata":{"id":"3-etN9RgoK1p"}},{"cell_type":"markdown","source":["in the code below, we get the data files from the google drive of the project"],"metadata":{"id":"4GC5k7zgqYUO"}},{"cell_type":"code","execution_count":4,"metadata":{"id":"9gjRGnA-QIJ2","executionInfo":{"status":"ok","timestamp":1685441908720,"user_tz":-120,"elapsed":343,"user":{"displayName":"GIOVANNI SCORZIELLO","userId":"08861605982515496649"}}},"outputs":[],"source":["# Da eseguire se hai caricato il dataset a mano nel runtime\n","mop_instances = \"/content/drive/Shareddrives/se4ai/mop.csv\"\n","aop_instances = \"/content/drive/Shareddrives/se4ai/aop.csv\"\n","nic_instances = \"/content/drive/Shareddrives/se4ai/nic.csv\""]},{"cell_type":"markdown","source":["The training of the model will be carried out through supervised learning, i.e. labels are added to the data which will allow the model to understand what type of information it is analyzing, the labels are then transformed into numbers, so as to be able to be more easily included by the model.\n","The model must be able to recognize 4 different types of linguistic antipattern within python code, they are:\n","- **\"Get\" More Than an Accessor**: A getter that performs actions other than returning the corresponding attribute.\n","- **Not Implemented Condition**: The comments of a method suggest a conditional behavior that is not implemented in the code. When the implementation is default this should be documented.\n","- **Attribute Signature and Comment are Opposite**: The declaration of an attribute is in contradiction with its documentation.\n","- **Method Signature and Comment are Opposite**: The  declaration of a method is in contradiction with its documentation.\n","\n","In addition to the 4 types of Antipattern, the model must also be able to recognize the instances of code that do not contain them, thus classifying them as instances of clean code (**Clear**)\n","\n","The labels assigned to the various code snippets, based on the antipattern they contain, within the dataset are:\n","- **mop**: used for \"Metohd Signature and Comment are Opposite\", this label will be assigned the value **0** for model training\n","- **aop**: used for \"Attribute Signature and Comment are Opposite\", this label will be assigned the value **1** for model training\n","- **clr**: used for \"Clear\", this label will be assigned the value **2** for model training\n","- **nic**: used for \"Not Implemented Condition\", this label will be assigned the value **3** for model training\n","- **get**: used for \"Get\" More than an Accessor, this label will be assigned the value **4** for model training\n"],"metadata":{"id":"y0jq7SHbEzlG"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"zJZrmEk9qZOd","executionInfo":{"status":"ok","timestamp":1685441913837,"user_tz":-120,"elapsed":1890,"user":{"displayName":"GIOVANNI SCORZIELLO","userId":"08861605982515496649"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ebc2f3a3-a39e-465e-c320-af42232d295e"},"outputs":[{"output_type":"stream","name":"stderr","text":["<frozen importlib._bootstrap>:914: ImportWarning: APICoreClientInfoImportHook.find_spec() not found; falling back to find_module()\n","<frozen importlib._bootstrap>:914: ImportWarning: _PyDriveImportHook.find_spec() not found; falling back to find_module()\n","<frozen importlib._bootstrap>:914: ImportWarning: _OpenCVImportHook.find_spec() not found; falling back to find_module()\n","<frozen importlib._bootstrap>:914: ImportWarning: _BokehImportHook.find_spec() not found; falling back to find_module()\n","<frozen importlib._bootstrap>:914: ImportWarning: _AltairImportHook.find_spec() not found; falling back to find_module()\n"]}],"source":["mop = pd.read_csv(mop_instances) # pandas read_csv legge automaticamente file csv e crea un \"oggetto\"\n","aop = pd.read_csv(aop_instances)\n","nic = pd.read_csv(nic_instances)\n","\n","# Creiamo array di label e array di snippet di codice\n","mop_labels = list([])\n","aop_labels = list([])\n","nic_labels = list([])\n","for l in list(mop['label']):\n","  if l == 'mop':\n","    mop_labels.append(0)\n","  else:\n","    mop_labels.append(2)\n","\n","for l in list(aop['label']):\n","  if l == 'aop':\n","    aop_labels.append(1)\n","  else:\n","    aop_labels.append(2)\n","\n","for l in list(nic['label']):\n","  if l == 'nic':\n","    nic_labels.append(3)\n","  else:\n","    nic_labels.append(2)\n","\n","\n","\n","mop_code = list(mop['code'])\n","aop_code = list(aop['code'])\n","nic_code = list(nic['code'])"]},{"cell_type":"markdown","source":["## Split data for Training and Testing"],"metadata":{"id":"_Pjqp598oSIx"}},{"cell_type":"code","execution_count":6,"metadata":{"id":"zaf1bSFfOgiW","executionInfo":{"status":"ok","timestamp":1685441926526,"user_tz":-120,"elapsed":1252,"user":{"displayName":"GIOVANNI SCORZIELLO","userId":"08861605982515496649"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"be98baa0-a5cd-4c21-f749-b1abb727d0e8"},"outputs":[{"output_type":"stream","name":"stderr","text":["<frozen importlib._bootstrap>:914: ImportWarning: APICoreClientInfoImportHook.find_spec() not found; falling back to find_module()\n","<frozen importlib._bootstrap>:914: ImportWarning: _PyDriveImportHook.find_spec() not found; falling back to find_module()\n","<frozen importlib._bootstrap>:914: ImportWarning: _OpenCVImportHook.find_spec() not found; falling back to find_module()\n","<frozen importlib._bootstrap>:914: ImportWarning: _BokehImportHook.find_spec() not found; falling back to find_module()\n","<frozen importlib._bootstrap>:914: ImportWarning: _AltairImportHook.find_spec() not found; falling back to find_module()\n","<frozen importlib._bootstrap>:914: ImportWarning: APICoreClientInfoImportHook.find_spec() not found; falling back to find_module()\n","<frozen importlib._bootstrap>:914: ImportWarning: _PyDriveImportHook.find_spec() not found; falling back to find_module()\n","<frozen importlib._bootstrap>:914: ImportWarning: _OpenCVImportHook.find_spec() not found; falling back to find_module()\n","<frozen importlib._bootstrap>:914: ImportWarning: _BokehImportHook.find_spec() not found; falling back to find_module()\n","<frozen importlib._bootstrap>:914: ImportWarning: _AltairImportHook.find_spec() not found; falling back to find_module()\n","<frozen importlib._bootstrap>:914: ImportWarning: APICoreClientInfoImportHook.find_spec() not found; falling back to find_module()\n","<frozen importlib._bootstrap>:914: ImportWarning: _PyDriveImportHook.find_spec() not found; falling back to find_module()\n","<frozen importlib._bootstrap>:914: ImportWarning: _OpenCVImportHook.find_spec() not found; falling back to find_module()\n","<frozen importlib._bootstrap>:914: ImportWarning: _BokehImportHook.find_spec() not found; falling back to find_module()\n","<frozen importlib._bootstrap>:914: ImportWarning: _AltairImportHook.find_spec() not found; falling back to find_module()\n","<frozen importlib._bootstrap>:914: ImportWarning: APICoreClientInfoImportHook.find_spec() not found; falling back to find_module()\n","<frozen importlib._bootstrap>:914: ImportWarning: _PyDriveImportHook.find_spec() not found; falling back to find_module()\n","<frozen importlib._bootstrap>:914: ImportWarning: _OpenCVImportHook.find_spec() not found; falling back to find_module()\n","<frozen importlib._bootstrap>:914: ImportWarning: _BokehImportHook.find_spec() not found; falling back to find_module()\n","<frozen importlib._bootstrap>:914: ImportWarning: _AltairImportHook.find_spec() not found; falling back to find_module()\n"]}],"source":["from sklearn.model_selection import train_test_split # funzione in sklearn per dividere il dataset in train, test, validation\n","# Divide il dataset in train test validation\n","# Parametri test_size:\n","# valori di default: train test_size = 0.33 val test_size = 0.3\n","TRAIN_TEST_SIZE = 0.33;\n","VAL_TEST_SIZE = 0.3;\n","# Tendenzialmente dovrebbero essere simili così train = 70%, test = 30%, val = 30% di test;\n","# Alzando test e val può migliorare accuracy perché ha più esempi per test e validazione\n","\n","mop_train_codes, mop_temp_codes, mop_train_labels, mop_temp_labels = train_test_split(mop_code, mop_labels, test_size = TRAIN_TEST_SIZE, shuffle = True, stratify = mop_labels);\n","aop_train_codes, aop_temp_codes, aop_train_labels, aop_temp_labels = train_test_split(aop_code, aop_labels, test_size = TRAIN_TEST_SIZE, shuffle = True, stratify = aop_labels);\n","nic_train_codes, nic_temp_codes, nic_train_labels, nic_temp_labels = train_test_split(nic_code, nic_labels, test_size = TRAIN_TEST_SIZE, shuffle = True, stratify = nic_labels);\n","\n","train_codes = mop_train_codes + aop_train_codes + nic_train_codes\n","temp_codes = mop_temp_codes +  aop_temp_codes + nic_temp_codes\n","train_labels = mop_train_labels + aop_train_labels + nic_train_labels\n","temp_labels = mop_temp_labels + aop_temp_labels + nic_temp_labels\n","\n","test_codes, val_codes, test_labels, val_labels = train_test_split(temp_codes, temp_labels, test_size = VAL_TEST_SIZE, shuffle = True, stratify = temp_labels );\n","\n"]},{"cell_type":"markdown","source":["# Model Evaluations Metrics\n","\n"],"metadata":{"id":"0sUdv5RLjb1p"}},{"cell_type":"markdown","source":["## Metrics Calculation\n","\n","Let's now import the functions that calculate the metrics of our model, through which we will be able to understand how efficient it is in determining whether an instance of code to be analyzed belongs to a certain class or to another.\n","The Metrics used to evaluate our model are:\n","\n","- **Accuracy**: It gives you the overall accuracy of the model, meaning the fraction of the total samples that were correctly classified by the classifier. To calculate accuracy, use the following formula: (TP+TN)/(TP+TN+FP+FN).\n","- **Precision**: It tells you what fraction of predictions as a positive class were actually positive. To calculate precision, use the following formula: TP/(TP+FP).\n","- **Recall**: It tells you what fraction of all positive samples were correctly predicted as positive by the classifier. It is also known as True Positive Rate (TPR), Sensitivity, Probability of Detection. To calculate Recall, use the following formula: TP/(TP+FN).\n","- **Specificity**: It tells you what fraction of all negative samples are correctly predicted as negative by the classifier. It is also known as True Negative Rate (TNR). To calculate specificity, use the following formula: TN/(TN+FP).\n","- **F1 Score**: It combines precision and recall into a single measure. Mathematically it’s the harmonic mean of precision and recall."],"metadata":{"id":"QEII1_OMopQX"}},{"cell_type":"markdown","source":["installiamo la libreria **evaluate** di hugging face\n","\n"],"metadata":{"id":"euqxOW2bONAO"}},{"cell_type":"code","source":["!pip install evaluate"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z6-APqJ2LL6e","executionInfo":{"status":"ok","timestamp":1685442074072,"user_tz":-120,"elapsed":15015,"user":{"displayName":"GIOVANNI SCORZIELLO","userId":"08861605982515496649"}},"outputId":"8162d643-1a76-46c2-bbf2-8831976fbbef"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting evaluate\n","  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets>=2.0.0 (from evaluate)\n","  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.22.4)\n","Collecting dill (from evaluate)\n","  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.27.1)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.65.0)\n","Collecting xxhash (from evaluate)\n","  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from evaluate)\n","  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.4.0)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.14.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.1)\n","Collecting responses<0.19 (from evaluate)\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n","Collecting aiohttp (from datasets>=2.0.0->evaluate)\n","  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2022.7.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n","Collecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.0.0->evaluate)\n","  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets>=2.0.0->evaluate)\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting yarl<2.0,>=1.0 (from aiohttp->datasets>=2.0.0->evaluate)\n","  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.0.0->evaluate)\n","  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.0.0->evaluate)\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n","Installing collected packages: xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, aiosignal, aiohttp, datasets, evaluate\n","Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.12.0 dill-0.3.6 evaluate-0.4.0 frozenlist-1.3.3 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.9.2\n"]},{"output_type":"stream","name":"stderr","text":["<frozen importlib._bootstrap>:914: ImportWarning: APICoreClientInfoImportHook.find_spec() not found; falling back to find_module()\n","<frozen importlib._bootstrap>:914: ImportWarning: _PyDriveImportHook.find_spec() not found; falling back to find_module()\n","<frozen importlib._bootstrap>:914: ImportWarning: _OpenCVImportHook.find_spec() not found; falling back to find_module()\n","<frozen importlib._bootstrap>:914: ImportWarning: _BokehImportHook.find_spec() not found; falling back to find_module()\n","<frozen importlib._bootstrap>:914: ImportWarning: _AltairImportHook.find_spec() not found; falling back to find_module()\n","/usr/local/lib/python3.10/dist-packages/google/colab/_pip.py:85: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.10/dist-packages/aiohttp-3.8.4.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n","  for line in open(toplevel):\n","ResourceWarning: Enable tracemalloc to get the object allocation traceback\n","/usr/local/lib/python3.10/dist-packages/google/colab/_pip.py:85: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.10/dist-packages/aiosignal-1.3.1.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n","  for line in open(toplevel):\n","ResourceWarning: Enable tracemalloc to get the object allocation traceback\n","/usr/local/lib/python3.10/dist-packages/google/colab/_pip.py:85: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.10/dist-packages/async_timeout-4.0.2.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n","  for line in open(toplevel):\n","ResourceWarning: Enable tracemalloc to get the object allocation traceback\n","/usr/local/lib/python3.10/dist-packages/google/colab/_pip.py:85: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.10/dist-packages/datasets-2.12.0.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n","  for line in open(toplevel):\n","ResourceWarning: Enable tracemalloc to get the object allocation traceback\n","/usr/local/lib/python3.10/dist-packages/google/colab/_pip.py:85: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.10/dist-packages/dill-0.3.6.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n","  for line in open(toplevel):\n","ResourceWarning: Enable tracemalloc to get the object allocation traceback\n","/usr/local/lib/python3.10/dist-packages/google/colab/_pip.py:85: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.10/dist-packages/evaluate-0.4.0.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n","  for line in open(toplevel):\n","ResourceWarning: Enable tracemalloc to get the object allocation traceback\n","/usr/local/lib/python3.10/dist-packages/google/colab/_pip.py:85: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.10/dist-packages/frozenlist-1.3.3.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n","  for line in open(toplevel):\n","ResourceWarning: Enable tracemalloc to get the object allocation traceback\n","/usr/local/lib/python3.10/dist-packages/google/colab/_pip.py:85: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.10/dist-packages/multidict-6.0.4.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n","  for line in open(toplevel):\n","ResourceWarning: Enable tracemalloc to get the object allocation traceback\n","/usr/local/lib/python3.10/dist-packages/google/colab/_pip.py:85: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.10/dist-packages/multiprocess-0.70.14.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n","  for line in open(toplevel):\n","ResourceWarning: Enable tracemalloc to get the object allocation traceback\n","/usr/local/lib/python3.10/dist-packages/google/colab/_pip.py:85: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.10/dist-packages/responses-0.18.0.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n","  for line in open(toplevel):\n","ResourceWarning: Enable tracemalloc to get the object allocation traceback\n","/usr/local/lib/python3.10/dist-packages/google/colab/_pip.py:85: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.10/dist-packages/xxhash-3.2.0.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n","  for line in open(toplevel):\n","ResourceWarning: Enable tracemalloc to get the object allocation traceback\n","/usr/local/lib/python3.10/dist-packages/google/colab/_pip.py:85: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.10/dist-packages/yarl-1.9.2.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n","  for line in open(toplevel):\n","ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"]}]},{"cell_type":"markdown","source":["We import the metrics with an avarage of type \"macro\".\n","This step is very important as the evaluation metrics of this library are default for binary classification problems, in our case we need to set them in such a way that we have some overall evaluation metrics to evaluate the efficiency for a classification model multiclass, and this is done by setting the average to \"macro\""],"metadata":{"id":"0Ar6IcetROkm"}},{"cell_type":"code","source":["import evaluate\n","\n","accuracy = evaluate.load(\"accuracy\", average=\"macro\")\n","precision = evaluate.load(\"precision\", average=\"macro\")\n","f1 = evaluate.load(\"f1\", average=\"macro\")\n","recall = evaluate.load(\"recall\", average=\"macro\")\n","mcc = evaluate.load(\"matthews_correlation\", avarage=\"macro\")"],"metadata":{"id":"aQ4oabrEoylo","executionInfo":{"status":"ok","timestamp":1685442564124,"user_tz":-120,"elapsed":2817,"user":{"displayName":"GIOVANNI SCORZIELLO","userId":"08861605982515496649"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["# Model Inizialization"],"metadata":{"id":"qndDXBKWoYln"}},{"cell_type":"markdown","source":["## Roberta Tokenizer\n","\n","We download the HuggingFace tokenization module which selects the tokenizer of the ML model **Roberta**, as it is the model on which CodeBERT is based.\n","It splits text into tokens using the WordPiece algorithm, which breaks words into smaller, more common parts (subwords), so you can handle unknown or infrequent words during training."],"metadata":{"id":"G-UhiR0XfN30"}},{"cell_type":"code","source":["from transformers import RobertaTokenizer\n","\n","tokenizer = RobertaTokenizer.from_pretrained('microsoft/codebert-base', do_lower_case = True);"],"metadata":{"id":"NJDeDvfiq64E","colab":{"base_uri":"https://localhost:8080/","height":177,"referenced_widgets":["f72df88fed884e7b97314368a982fbae","191bbd30f97e489eab9b45187d8464a6","d56d3b675a594afa82e97d2d9d97c20a","0c90b6bb34de49329f7452a726297a7b","5f5e1ddd1690465386d904617268aab1","9136c7d145be4ec59797638ebee61c7f","0ebe36dc8aef44a3964ea0f65daee1b5","f5e0cf45e9014f45b65a6ad40a454e35","41ec0c7ad0a342229c22d749b1ab1aec","a56185ce2ae1412f9646e91532e3a125","e15878edd30f4245bbf9ee7131234441","397aa5e7e1724e53bde97836798af8ef","da59115be3a849059c8daad51c49cee9","ec4f2da0966e4e87ac8b4d36c6c42a4c","389a76ec22d24fb68f207778eb657c17","98e631967e2841d5b86c82b6e8351c87","e6dd79230bf648c0a3c0b108b0e6a269","522fad1f69a040dc8060988d2a254aee","48e9e3dcbdeb4578ab4859a402882ee3","a7f83253b9d14c778139b2de737a9028","cbe090b3ce2740b4b3523a6bc77acabf","0989a1c863ba48a4816f306449f1efc1","65382a01323f4f39b43b8c0853a999d8","fe123d7d953e4df88326ba4fd983d011","ee9d7315390f40f2b16d934c5f1035ec","72242088179e4c07bf4378704755ee94","449391d978fd4090bcf819e2abcc32fb","9814106b063846839db4ed36776e371c","33e431f3c9e546df8c47903dd88e3e21","99f91a2aeb064d84af2d2ed638ed0581","75763e3e63274a939749a735da41661c","ad40377798734cd5922b5dfb6f0946c4","ecf209dbd61c4385b6272ca8d5febc28","73145279f67c4cdb90942dcf6cc5d4fe","45001572193b46c0b5a0df644bdc8e32","80d71a6d85cc446c9f996c6abe6340bc","05a75db33f3846f0b102c87fc7a630c8","64aad1b7f58849eb9e2aeae48863b8ea","f9daf7f9869b49758cd479a914f98a44","687be3d975634437afd4aa59aa8045dc","f54ccb746f744468ae8e7ee6a0170c4c","36fe258b872747e7b8c068dc7e99face","9dfc05a1afc845f5aaa58aebe0eddf10","9d2ee4ffc0b947a79717b0af84dee8d9","a13969cb3c1f4eb888f9d438767c2471","5723d901e2434772b402c45621448052","17856e7f952640d0bfd6170b25e2699e","24095d2602654379b1a496a64ffd84a6","fdbe5c61a5044fad9a7fc0f592febefa","0642c8778fd746d8a63d6de76372bda5","c4d55917fd4749ac99889ebe27f312fc","9ae85f65f5354a8db6b58f7df505a5db","424f130407b541428fa019b450b8b049","7f9777ef4f344f3ea0b298af6f6eb2db","1dae1b1989614ca2be112e0f0cb786a5"]},"executionInfo":{"status":"ok","timestamp":1685442110224,"user_tz":-120,"elapsed":2163,"user":{"displayName":"GIOVANNI SCORZIELLO","userId":"08861605982515496649"}},"outputId":"cd07eb96-a619-4cc4-c178-9c7ef312bf8c"},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f72df88fed884e7b97314368a982fbae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"397aa5e7e1724e53bde97836798af8ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65382a01323f4f39b43b8c0853a999d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73145279f67c4cdb90942dcf6cc5d4fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/498 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a13969cb3c1f4eb888f9d438767c2471"}},"metadata":{}}]},{"cell_type":"markdown","source":["## Roberta for Sequesnce Classification\n","\n","We download the module through which it is possible to select the Machine Learning models that are based on the **Roberta** model, as in our case for codeBERT.\n","\n","With this specific module the model is loaded by adding a linear level above the **pooled output**, which is nothing more than an attention head for sequence classification/regression.\n","CodeBERT is in fact a model that allows you to perform feature extraction operations, while our model must be able to perform code classification operations, hence the need to train it to perform an operation different from its base, and therefore to add a new attention head."],"metadata":{"id":"n--I6nGTfnp9"}},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":156,"referenced_widgets":["7561661529c14d33b2279b2f4c0d05d9","fe6f2c439e9d477695040db56a44ed8a","d12190b672c94f6dacf44474c9ce7b9a","9690f5db36454fd096700182b8af2a5e","81a70830671042a59c1a3555facd612d","3be684b9d14a479094d9a4ae5e24a1ce","dc8ef5dfa1134905897b9fc88a040503","988a9ec4b91a43dd9f0e35f3a6ed861a","a27441962f5a4468926d65fd1f77a8d4","152e8f6801624d2e95fc2ed18785ac05","6980c3464f9448dca5b65f55d02b8f6b"]},"id":"UaiZiEJ7qorV","outputId":"3098d5e1-bfc6-4821-ee2f-f5a774aaa84e","executionInfo":{"status":"ok","timestamp":1685442117642,"user_tz":-120,"elapsed":4452,"user":{"displayName":"GIOVANNI SCORZIELLO","userId":"08861605982515496649"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7561661529c14d33b2279b2f4c0d05d9"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import RobertaForSequenceClassification\n","# Quando esegui per la prima volta (nello stesso runtime) li deve scaricare.\n","# Dopo aver eseguito returna questo warning:\n","# ---\n","# Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n","# This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","# ---\n","# Noi stiamo facendo esattamente quello che dice quindi va bene.\n","tokenizer = RobertaTokenizer.from_pretrained('microsoft/codebert-base', do_lower_case = True);\n","\n","id2label = {0: \"mop\", 2: \"clr\", 1: \"aop\", 3: \"nic\"}\n","label2id = {\"mop\": 0, \"clr\": 2, \"aop\": 1, \"nic\": 3}\n","\n","# Attenzione al parametro num_labels in base a quante label si devono classificare (4 = aop, clr, mop, nic)\n","model = RobertaForSequenceClassification.from_pretrained('microsoft/codebert-base', num_labels = 4, id2label=id2label, label2id=label2id)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"XA2hSfcNOvAU","executionInfo":{"status":"ok","timestamp":1685442124269,"user_tz":-120,"elapsed":1586,"user":{"displayName":"GIOVANNI SCORZIELLO","userId":"08861605982515496649"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b1ee3514-b87c-4b80-b4d7-35d76abb8b60"},"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]}],"source":["def preprocessing(input_text, tokenizer):\n","  '''\n","  Returns <class transformers.tokenization_utils_base.BatchEncoding> with the following fields:\n","    - input_ids: list of token ids\n","    - token_type_ids: list of token type ids\n","    - attention_mask: list of indices (0,1) specifying which tokens should considered by the model (return_attention_mask = True).\n","  '''\n","  return tokenizer.encode_plus(\n","                        input_text,\n","                        add_special_tokens = True,\n","                        max_length = 90,\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,\n","                        return_tensors = 'pt'\n","                   )\n","\n","def preprocessing_batch(data_set):\n","    token_id = []\n","    attention_masks = []\n","    for sample in data_set:\n","      encoding_dict = preprocessing(sample, tokenizer)\n","      token_id.append(encoding_dict['input_ids']) \n","      attention_masks.append(encoding_dict['attention_mask'])\n","    token_id = torch.cat(token_id, dim = 0)\n","    attention_masks = torch.cat(attention_masks, dim = 0)\n","    return token_id,attention_masks;\n","\n","train_token_id,train_attention_masks = preprocessing_batch(train_codes);\n","test_token_id,test_attention_masks = preprocessing_batch(test_codes);\n","val_token_id,val_attention_masks = preprocessing_batch(val_codes);\n","\n","def print_rand_sentence_encoding(text, token_id):\n","  '''Displays tokens, token IDs and attention mask of a random text sample'''\n","  index = random.randint(0, len(text) - 1)\n","  tokens = tokenizer.tokenize(tokenizer.decode(token_id[index]))\n","  token_ids = [i.numpy() for i in token_id[index]]\n","  print(tokens);\n","  table = np.array([tokens, token_ids]).T\n","  print(tabulate(table, \n","                 headers = ['Tokens', 'Token IDs'],\n","                 tablefmt = 'fancy_grid'))\n"]},{"cell_type":"markdown","source":["We take the labels of our dataset that we have divided into different samples for training, testing and validation and transform them into Tensors"],"metadata":{"id":"wW455tegf6gt"}},{"cell_type":"code","execution_count":14,"metadata":{"id":"72_txlYlTsBp","executionInfo":{"status":"ok","timestamp":1685442128556,"user_tz":-120,"elapsed":390,"user":{"displayName":"GIOVANNI SCORZIELLO","userId":"08861605982515496649"}}},"outputs":[],"source":["train_labels = torch.tensor(train_labels)\n","test_labels = torch.tensor(test_labels)\n","val_labels = torch.tensor(val_labels)"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"Uem77tH5WLuO","executionInfo":{"status":"ok","timestamp":1685442130872,"user_tz":-120,"elapsed":1,"user":{"displayName":"GIOVANNI SCORZIELLO","userId":"08861605982515496649"}}},"outputs":[],"source":["batch_size = 16\n","\n","train_set = TensorDataset(train_token_id, \n","                          train_attention_masks, \n","                          train_labels)\n","\n","val_set = TensorDataset(val_token_id, \n","                        val_attention_masks, \n","                        val_labels)\n","\n","test_set = TensorDataset(test_token_id, \n","                        test_attention_masks, \n","                        test_labels)\n","\n","train_dataloader = DataLoader(\n","            train_set,\n","            sampler = RandomSampler(train_set),\n","            batch_size = batch_size\n","        )\n","\n","validation_dataloader = DataLoader(\n","            val_set,\n","            sampler = SequentialSampler(val_set),\n","            batch_size = batch_size\n","        )\n","\n","\n","test_dataloader = DataLoader(\n","            test_set,\n","            sampler = SequentialSampler(test_set),\n","            batch_size = batch_size\n","        )"]},{"cell_type":"markdown","source":["# Optimization\n","\n","Let's optimize the parameters of our model in order to obtain the best possible result for the fine tuning, specifically let's modify:\n","1. **Learning Rate (lr)**: a hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated. In our case it was set to 5e-5\n","2. **Epsilon**: used as a guard against a by zero division. In our case it was set to 1e-08\n","3. **Weight Decay**: a regularization technique by adding a small penalty, usually the L2 norm of the weights (all the weights of the model), to the loss function. In our case it was set to 0.01"],"metadata":{"id":"Sp8VpJg5o9nt"}},{"cell_type":"code","execution_count":16,"metadata":{"id":"lvd66AwUW1GV","executionInfo":{"status":"ok","timestamp":1685442135274,"user_tz":-120,"elapsed":1553,"user":{"displayName":"GIOVANNI SCORZIELLO","userId":"08861605982515496649"}}},"outputs":[],"source":["optimizer = torch.optim.AdamW(model.parameters(), \n","                              lr = 5e-5, # LEARNING RATE DELL'ALGORITMO OTTIMIZZATORE (2e-5 = 2*10^-5 = 0.00005)\n","                              eps = 1e-08,\n","                              weight_decay = 0.01\n","                              )\n","model.cuda(); # Eseguire per impostare il modello in modo da usare la GPU durante training"]},{"cell_type":"markdown","source":["# Training\n","\n","The training of the model is carried out in this block, it has been set with a batch size of 16 and 8 epochs, for each batch of each epoch the metrics calculated for that training are printed on the screen.\n","At the end of the Training we will have a model developed to carry out classification operations, and capable of recognizing the linguistic antipatterns within the python code described above."],"metadata":{"id":"6kU_NH0UpENI"}},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dXlSKiTQXB8g","outputId":"93d77e9e-477d-48e9-87bf-a5427d6a7a60","executionInfo":{"status":"ok","timestamp":1685442933100,"user_tz":-120,"elapsed":264543,"user":{"displayName":"GIOVANNI SCORZIELLO","userId":"08861605982515496649"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["\rEpoch:   0%|          | 0/8 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","\t - Train loss: 0.0838\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.735994\n","\t - Precision: 0.750000\n","\t - Recall: 0.729167\n","\n","\t - Train loss: 0.0838\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.614583\n","\t - Precision: 0.566667\n","\t - Recall: 0.812500\n","\n","\t - Train loss: 0.0838\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.0838\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.843750\n","\t - Precision: 0.944444\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0838\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.781593\n","\t - Precision: 0.770833\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0838\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.0786\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.735994\n","\t - Precision: 0.750000\n","\t - Recall: 0.729167\n","\n","\t - Train loss: 0.0786\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.718137\n","\t - Precision: 0.697619\n","\t - Recall: 0.837500\n","\n","\t - Train loss: 0.0786\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.0786\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.843750\n","\t - Precision: 0.944444\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0786\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.781593\n","\t - Precision: 0.770833\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0786\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.1620\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.735994\n","\t - Precision: 0.750000\n","\t - Recall: 0.729167\n","\n","\t - Train loss: 0.1620\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.718137\n","\t - Precision: 0.697619\n","\t - Recall: 0.837500\n","\n","\t - Train loss: 0.1620\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.1620\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.843750\n","\t - Precision: 0.944444\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.1620\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.781593\n","\t - Precision: 0.770833\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.1620\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.1445\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.735994\n","\t - Precision: 0.750000\n","\t - Recall: 0.729167\n","\n","\t - Train loss: 0.1445\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.718137\n","\t - Precision: 0.697619\n","\t - Recall: 0.837500\n","\n","\t - Train loss: 0.1445\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.1445\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.843750\n","\t - Precision: 0.944444\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.1445\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.781593\n","\t - Precision: 0.770833\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.1445\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.1593\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.735994\n","\t - Precision: 0.750000\n","\t - Recall: 0.729167\n","\n","\t - Train loss: 0.1593\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.718137\n","\t - Precision: 0.697619\n","\t - Recall: 0.837500\n","\n","\t - Train loss: 0.1593\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.1593\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.843750\n","\t - Precision: 0.944444\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.1593\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.781593\n","\t - Precision: 0.770833\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.1593\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.1479\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.735994\n","\t - Precision: 0.750000\n","\t - Recall: 0.729167\n","\n","\t - Train loss: 0.1479\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.718137\n","\t - Precision: 0.697619\n","\t - Recall: 0.837500\n","\n","\t - Train loss: 0.1479\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.1479\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.843750\n","\t - Precision: 0.944444\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.1479\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.781593\n","\t - Precision: 0.770833\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.1479\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.1299\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.735994\n","\t - Precision: 0.750000\n","\t - Recall: 0.729167\n","\n","\t - Train loss: 0.1299\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.718137\n","\t - Precision: 0.697619\n","\t - Recall: 0.837500\n","\n","\t - Train loss: 0.1299\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.1299\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.843750\n","\t - Precision: 0.944444\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.1299\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.781593\n","\t - Precision: 0.770833\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.1299\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.1508\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.735994\n","\t - Precision: 0.750000\n","\t - Recall: 0.729167\n","\n","\t - Train loss: 0.1508\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.718137\n","\t - Precision: 0.697619\n","\t - Recall: 0.837500\n","\n","\t - Train loss: 0.1508\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.1508\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.843750\n","\t - Precision: 0.944444\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.1508\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.781593\n","\t - Precision: 0.770833\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.1508\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.1410\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.735994\n","\t - Precision: 0.750000\n","\t - Recall: 0.729167\n","\n","\t - Train loss: 0.1410\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.777778\n","\t - Precision: 0.743750\n","\t - Recall: 0.862500\n","\n","\t - Train loss: 0.1410\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.1410\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.1410\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.872619\n","\t - Precision: 0.854167\n","\t - Recall: 0.928571\n","\n","\t - Train loss: 0.1410\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.1284\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.735994\n","\t - Precision: 0.750000\n","\t - Recall: 0.729167\n","\n","\t - Train loss: 0.1284\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.600877\n","\t - Precision: 0.594444\n","\t - Recall: 0.612500\n","\n","\t - Train loss: 0.1284\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.1284\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.1284\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.872619\n","\t - Precision: 0.854167\n","\t - Recall: 0.928571\n","\n","\t - Train loss: 0.1284\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.1183\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.735994\n","\t - Precision: 0.750000\n","\t - Recall: 0.729167\n","\n","\t - Train loss: 0.1183\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.600877\n","\t - Precision: 0.594444\n","\t - Recall: 0.612500\n","\n","\t - Train loss: 0.1183\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.1183\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.1183\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.872619\n","\t - Precision: 0.854167\n","\t - Recall: 0.928571\n","\n","\t - Train loss: 0.1183\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.1168\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.735994\n","\t - Precision: 0.750000\n","\t - Recall: 0.729167\n","\n","\t - Train loss: 0.1168\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.600877\n","\t - Precision: 0.594444\n","\t - Recall: 0.612500\n","\n","\t - Train loss: 0.1168\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.1168\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.1168\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.872619\n","\t - Precision: 0.854167\n","\t - Recall: 0.928571\n","\n","\t - Train loss: 0.1168\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.1271\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.735994\n","\t - Precision: 0.750000\n","\t - Recall: 0.729167\n","\n","\t - Train loss: 0.1271\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.600877\n","\t - Precision: 0.594444\n","\t - Recall: 0.612500\n","\n","\t - Train loss: 0.1271\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.1271\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.1271\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.872619\n","\t - Precision: 0.854167\n","\t - Recall: 0.928571\n","\n","\t - Train loss: 0.1271\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.1192\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.735994\n","\t - Precision: 0.750000\n","\t - Recall: 0.729167\n","\n","\t - Train loss: 0.1192\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.600877\n","\t - Precision: 0.594444\n","\t - Recall: 0.612500\n","\n","\t - Train loss: 0.1192\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.1192\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.1192\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.872619\n","\t - Precision: 0.854167\n","\t - Recall: 0.928571\n","\n","\t - Train loss: 0.1192\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.1176\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.735994\n","\t - Precision: 0.750000\n","\t - Recall: 0.729167\n","\n","\t - Train loss: 0.1176\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.600877\n","\t - Precision: 0.594444\n","\t - Recall: 0.612500\n","\n","\t - Train loss: 0.1176\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.1176\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.1176\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.1176\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.1113\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.777778\n","\t - Precision: 0.841667\n","\t - Recall: 0.760417\n","\n","\t - Train loss: 0.1113\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.1113\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.1113\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.1113\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.1113\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.1058\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.777778\n","\t - Precision: 0.841667\n","\t - Recall: 0.760417\n","\n","\t - Train loss: 0.1058\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.1058\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.1058\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.1058\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.1058\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.1042\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.777778\n","\t - Precision: 0.841667\n","\t - Recall: 0.760417\n","\n","\t - Train loss: 0.1042\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.1042\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.1042\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.1042\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.1042\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.1020\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.777778\n","\t - Precision: 0.841667\n","\t - Recall: 0.760417\n","\n","\t - Train loss: 0.1020\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.1020\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.1020\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.1020\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.1020\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.1002\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.777778\n","\t - Precision: 0.841667\n","\t - Recall: 0.760417\n","\n","\t - Train loss: 0.1002\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.1002\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.1002\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.1002\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.1002\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.0963\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.777778\n","\t - Precision: 0.841667\n","\t - Recall: 0.760417\n","\n","\t - Train loss: 0.0963\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0963\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.0963\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0963\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.0963\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.0926\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.777778\n","\t - Precision: 0.841667\n","\t - Recall: 0.760417\n","\n","\t - Train loss: 0.0926\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0926\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.0926\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0926\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.0926\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.0894\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.777778\n","\t - Precision: 0.841667\n","\t - Recall: 0.760417\n","\n","\t - Train loss: 0.0894\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0894\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.0894\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0894\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.0894\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.0864\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.777778\n","\t - Precision: 0.841667\n","\t - Recall: 0.760417\n","\n","\t - Train loss: 0.0864\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0864\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.0864\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0864\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.0864\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.0917\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.777778\n","\t - Precision: 0.841667\n","\t - Recall: 0.760417\n","\n","\t - Train loss: 0.0917\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0917\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.0917\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0917\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.0917\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.0911\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.735994\n","\t - Precision: 0.750000\n","\t - Recall: 0.729167\n","\n","\t - Train loss: 0.0911\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0911\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.0911\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0911\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.872619\n","\t - Precision: 0.854167\n","\t - Recall: 0.928571\n","\n","\t - Train loss: 0.0911\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.0882\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.735994\n","\t - Precision: 0.750000\n","\t - Recall: 0.729167\n","\n","\t - Train loss: 0.0882\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0882\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.0882\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0882\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.872619\n","\t - Precision: 0.854167\n","\t - Recall: 0.928571\n","\n","\t - Train loss: 0.0882\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.0855\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.735994\n","\t - Precision: 0.750000\n","\t - Recall: 0.729167\n","\n","\t - Train loss: 0.0855\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0855\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.0855\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0855\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.872619\n","\t - Precision: 0.854167\n","\t - Recall: 0.928571\n","\n","\t - Train loss: 0.0855\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.0832\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.735994\n","\t - Precision: 0.750000\n","\t - Recall: 0.729167\n","\n","\t - Train loss: 0.0832\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0832\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.0832\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0832\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.872619\n","\t - Precision: 0.854167\n","\t - Recall: 0.928571\n","\n","\t - Train loss: 0.0832\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.0860\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.735994\n","\t - Precision: 0.750000\n","\t - Recall: 0.729167\n","\n","\t - Train loss: 0.0860\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.676471\n","\t - Precision: 0.614286\n","\t - Recall: 0.837500\n","\n","\t - Train loss: 0.0860\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.0860\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.843750\n","\t - Precision: 0.944444\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0860\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.872619\n","\t - Precision: 0.854167\n","\t - Recall: 0.928571\n","\n","\t - Train loss: 0.0860\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.0838\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.735994\n","\t - Precision: 0.750000\n","\t - Recall: 0.729167\n","\n","\t - Train loss: 0.0838\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.676471\n","\t - Precision: 0.614286\n","\t - Recall: 0.837500\n","\n","\t - Train loss: 0.0838\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.0838\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.843750\n","\t - Precision: 0.944444\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0838\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.872619\n","\t - Precision: 0.854167\n","\t - Recall: 0.928571\n","\n","\t - Train loss: 0.0838\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.0816\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.735994\n","\t - Precision: 0.750000\n","\t - Recall: 0.729167\n","\n","\t - Train loss: 0.0816\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.676471\n","\t - Precision: 0.614286\n","\t - Recall: 0.837500\n","\n","\t - Train loss: 0.0816\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.0816\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.843750\n","\t - Precision: 0.944444\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0816\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.872619\n","\t - Precision: 0.854167\n","\t - Recall: 0.928571\n","\n","\t - Train loss: 0.0816\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.0800\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.735994\n","\t - Precision: 0.750000\n","\t - Recall: 0.729167\n","\n","\t - Train loss: 0.0800\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.676471\n","\t - Precision: 0.614286\n","\t - Recall: 0.837500\n","\n","\t - Train loss: 0.0800\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.0800\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.843750\n","\t - Precision: 0.944444\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0800\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.872619\n","\t - Precision: 0.854167\n","\t - Recall: 0.928571\n","\n","\t - Train loss: 0.0800\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.0781\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.735994\n","\t - Precision: 0.750000\n","\t - Recall: 0.729167\n","\n","\t - Train loss: 0.0781\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.676471\n","\t - Precision: 0.614286\n","\t - Recall: 0.837500\n","\n","\t - Train loss: 0.0781\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.0781\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.843750\n","\t - Precision: 0.944444\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0781\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.872619\n","\t - Precision: 0.854167\n","\t - Recall: 0.928571\n","\n","\t - Train loss: 0.0781\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.0773\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.735994\n","\t - Precision: 0.750000\n","\t - Recall: 0.729167\n","\n","\t - Train loss: 0.0773\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.676471\n","\t - Precision: 0.614286\n","\t - Recall: 0.837500\n","\n","\t - Train loss: 0.0773\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.0773\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.843750\n","\t - Precision: 0.944444\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0773\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.872619\n","\t - Precision: 0.854167\n","\t - Recall: 0.928571\n","\n","\t - Train loss: 0.0773\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.0770\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.735994\n","\t - Precision: 0.750000\n","\t - Recall: 0.729167\n","\n","\t - Train loss: 0.0770\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.676471\n","\t - Precision: 0.614286\n","\t - Recall: 0.837500\n","\n","\t - Train loss: 0.0770\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.0770\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.843750\n","\t - Precision: 0.944444\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0770\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.872619\n","\t - Precision: 0.854167\n","\t - Recall: 0.928571\n","\n","\t - Train loss: 0.0770\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.0763\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.735994\n","\t - Precision: 0.750000\n","\t - Recall: 0.729167\n","\n","\t - Train loss: 0.0763\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.676471\n","\t - Precision: 0.614286\n","\t - Recall: 0.837500\n","\n","\t - Train loss: 0.0763\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.0763\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.843750\n","\t - Precision: 0.944444\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0763\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.872619\n","\t - Precision: 0.854167\n","\t - Recall: 0.928571\n","\n","\t - Train loss: 0.0763\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.0748\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.735994\n","\t - Precision: 0.750000\n","\t - Recall: 0.729167\n","\n","\t - Train loss: 0.0748\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.676471\n","\t - Precision: 0.614286\n","\t - Recall: 0.837500\n","\n","\t - Train loss: 0.0748\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.0748\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.843750\n","\t - Precision: 0.944444\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0748\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.872619\n","\t - Precision: 0.854167\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch:  12%|█▎        | 1/8 [00:31<03:40, 31.55s/it]"]},{"output_type":"stream","name":"stdout","text":["\t - Recall: 0.928571\n","\n","\t - Train loss: 0.0748\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.1096\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.735994\n","\t - Precision: 0.750000\n","\t - Recall: 0.729167\n","\n","\t - Train loss: 0.1096\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.676471\n","\t - Precision: 0.614286\n","\t - Recall: 0.837500\n","\n","\t - Train loss: 0.1096\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.1096\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.843750\n","\t - Precision: 0.944444\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.1096\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.872619\n","\t - Precision: 0.854167\n","\t - Recall: 0.928571\n","\n","\t - Train loss: 0.1096\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.0895\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.659524\n","\t - Precision: 0.691667\n","\t - Recall: 0.645833\n","\n","\t - Train loss: 0.0895\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0895\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.0895\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.843750\n","\t - Precision: 0.944444\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0895\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.872619\n","\t - Precision: 0.854167\n","\t - Recall: 0.928571\n","\n","\t - Train loss: 0.0895\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.0656\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.659524\n","\t - Precision: 0.691667\n","\t - Recall: 0.645833\n","\n","\t - Train loss: 0.0656\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0656\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.0656\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.843750\n","\t - Precision: 0.944444\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0656\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.872619\n","\t - Precision: 0.854167\n","\t - Recall: 0.928571\n","\n","\t - Train loss: 0.0656\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.0560\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.700877\n","\t - Precision: 0.784091\n","\t - Recall: 0.677083\n","\n","\t - Train loss: 0.0560\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0560\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.0560\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.843750\n","\t - Precision: 0.944444\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0560\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.0560\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.0761\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.700877\n","\t - Precision: 0.784091\n","\t - Recall: 0.677083\n","\n","\t - Train loss: 0.0761\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0761\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.0761\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0761\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.0761\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.0650\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.700877\n","\t - Precision: 0.784091\n","\t - Recall: 0.677083\n","\n","\t - Train loss: 0.0650\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0650\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.0650\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0650\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.0650\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.0575\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.700877\n","\t - Precision: 0.784091\n","\t - Recall: 0.677083\n","\n","\t - Train loss: 0.0575\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0575\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.0575\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0575\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.0575\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.0618\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.700877\n","\t - Precision: 0.784091\n","\t - Recall: 0.677083\n","\n","\t - Train loss: 0.0618\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0618\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.0618\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0618\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.0618\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.0671\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.700877\n","\t - Precision: 0.784091\n","\t - Recall: 0.677083\n","\n","\t - Train loss: 0.0671\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0671\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.0671\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0671\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0671\n","\t - Accuracy: 0.666667\n","\t - F1 Score: 0.559524\n","\t - Precision: 0.583333\n","\t - Recall: 0.541667\n","\n","\t - Train loss: 0.0667\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.700877\n","\t - Precision: 0.784091\n","\t - Recall: 0.677083\n","\n","\t - Train loss: 0.0667\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0667\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.0667\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0667\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0667\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0618\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0618\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0618\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.0618\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0618\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0618\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0684\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0684\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0684\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.0684\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0684\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.735119\n","\t - Precision: 0.854167\n","\t - Recall: 0.714286\n","\n","\t - Train loss: 0.0684\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0645\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0645\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0645\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.0645\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0645\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.735119\n","\t - Precision: 0.854167\n","\t - Recall: 0.714286\n","\n","\t - Train loss: 0.0645\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0606\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0606\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0606\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.566667\n","\t - Precision: 0.591667\n","\t - Recall: 0.694444\n","\n","\t - Train loss: 0.0606\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0606\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.735119\n","\t - Precision: 0.854167\n","\t - Recall: 0.714286\n","\n","\t - Train loss: 0.0606\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0573\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0573\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0573\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.566667\n","\t - Precision: 0.591667\n","\t - Recall: 0.694444\n","\n","\t - Train loss: 0.0573\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.702068\n","\t - Precision: 0.895833\n","\t - Recall: 0.683333\n","\n","\t - Train loss: 0.0573\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.735119\n","\t - Precision: 0.854167\n","\t - Recall: 0.714286\n","\n","\t - Train loss: 0.0573\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0547\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0547\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0547\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.566667\n","\t - Precision: 0.591667\n","\t - Recall: 0.694444\n","\n","\t - Train loss: 0.0547\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.702068\n","\t - Precision: 0.895833\n","\t - Recall: 0.683333\n","\n","\t - Train loss: 0.0547\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.735119\n","\t - Precision: 0.854167\n","\t - Recall: 0.714286\n","\n","\t - Train loss: 0.0547\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0527\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0527\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0527\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.566667\n","\t - Precision: 0.591667\n","\t - Recall: 0.694444\n","\n","\t - Train loss: 0.0527\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.702068\n","\t - Precision: 0.895833\n","\t - Recall: 0.683333\n","\n","\t - Train loss: 0.0527\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.735119\n","\t - Precision: 0.854167\n","\t - Recall: 0.714286\n","\n","\t - Train loss: 0.0527\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0504\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0504\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0504\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.566667\n","\t - Precision: 0.591667\n","\t - Recall: 0.694444\n","\n","\t - Train loss: 0.0504\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.702068\n","\t - Precision: 0.895833\n","\t - Recall: 0.683333\n","\n","\t - Train loss: 0.0504\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.735119\n","\t - Precision: 0.854167\n","\t - Recall: 0.714286\n","\n","\t - Train loss: 0.0504\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0578\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0578\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0578\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.606250\n","\t - Precision: 0.616071\n","\t - Recall: 0.722222\n","\n","\t - Train loss: 0.0578\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.702068\n","\t - Precision: 0.895833\n","\t - Recall: 0.683333\n","\n","\t - Train loss: 0.0578\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.557423\n","\t - Precision: 0.587500\n","\t - Recall: 0.589286\n","\n","\t - Train loss: 0.0578\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0556\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0556\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0556\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.694444\n","\t - Precision: 0.777778\n","\t - Recall: 0.777778\n","\n","\t - Train loss: 0.0556\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.702068\n","\t - Precision: 0.895833\n","\t - Recall: 0.683333\n","\n","\t - Train loss: 0.0556\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.557423\n","\t - Precision: 0.587500\n","\t - Recall: 0.589286\n","\n","\t - Train loss: 0.0556\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0543\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0543\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0543\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.694444\n","\t - Precision: 0.777778\n","\t - Recall: 0.777778\n","\n","\t - Train loss: 0.0543\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.702068\n","\t - Precision: 0.895833\n","\t - Recall: 0.683333\n","\n","\t - Train loss: 0.0543\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.557423\n","\t - Precision: 0.587500\n","\t - Recall: 0.589286\n","\n","\t - Train loss: 0.0543\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0663\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0663\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0663\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.614583\n","\t - Precision: 0.728571\n","\t - Recall: 0.722222\n","\n","\t - Train loss: 0.0663\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.702068\n","\t - Precision: 0.895833\n","\t - Recall: 0.683333\n","\n","\t - Train loss: 0.0663\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.557423\n","\t - Precision: 0.587500\n","\t - Recall: 0.589286\n","\n","\t - Train loss: 0.0663\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0721\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0721\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0721\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.614583\n","\t - Precision: 0.728571\n","\t - Recall: 0.722222\n","\n","\t - Train loss: 0.0721\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.702068\n","\t - Precision: 0.895833\n","\t - Recall: 0.683333\n","\n","\t - Train loss: 0.0721\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.557423\n","\t - Precision: 0.587500\n","\t - Recall: 0.589286\n","\n","\t - Train loss: 0.0721\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0778\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0778\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0778\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.614583\n","\t - Precision: 0.728571\n","\t - Recall: 0.722222\n","\n","\t - Train loss: 0.0778\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0778\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.557423\n","\t - Precision: 0.587500\n","\t - Recall: 0.589286\n","\n","\t - Train loss: 0.0778\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0768\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0768\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0768\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.566667\n","\t - Precision: 0.591667\n","\t - Recall: 0.694444\n","\n","\t - Train loss: 0.0768\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0768\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.735119\n","\t - Precision: 0.854167\n","\t - Recall: 0.714286\n","\n","\t - Train loss: 0.0768\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0744\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0744\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0744\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.0744\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0744\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.735119\n","\t - Precision: 0.854167\n","\t - Recall: 0.714286\n","\n","\t - Train loss: 0.0744\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0734\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0734\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0734\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.0734\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0734\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.830952\n","\t - Precision: 0.875000\n","\t - Recall: 0.839286\n","\n","\t - Train loss: 0.0734\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0711\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0711\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0711\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.0711\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0711\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0711\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0726\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0726\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0726\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.0726\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0726\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0726\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0707\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0707\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0707\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.0707\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0707\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0707\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0689\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0689\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0689\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0689\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.843750\n","\t - Precision: 0.944444\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0689\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.0689\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0695\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0695\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0695\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0695\n","\t - Accuracy: 0.937500\n","\t - F1 Score: 0.933333\n","\t - Precision: 0.968750\n","\t - Recall: 0.916667\n","\n","\t - Train loss: 0.0695\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.0695\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0716\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0716\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0716\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0716\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.830952\n","\t - Precision: 0.839286\n","\t - Recall: 0.880952\n","\n","\t - Train loss: 0.0716\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.0716\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0699\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0699\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0699\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0699\n","\t - Accuracy: 0.875000\n","\t - F1 Score: 0.830952\n","\t - Precision: 0.839286\n","\t - Recall: 0.880952\n","\n","\t - Train loss: 0.0699\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.0699\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0683\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0683\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0683\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0683\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.788889\n","\t - Precision: 0.812500\n","\t - Recall: 0.830952\n","\n","\t - Train loss: 0.0683\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.0683\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0669\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0669\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0669\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0669\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.788889\n","\t - Precision: 0.812500\n","\t - Recall: 0.830952\n","\n","\t - Train loss: 0.0669\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.648810\n","\t - Precision: 0.687500\n","\t - Recall: 0.732143\n","\n","\t - Train loss: 0.0669\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0685\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.700877\n","\t - Precision: 0.825758\n","\t - Recall: 0.697917\n","\n","\t - Train loss: 0.0685\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0685\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0685\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.788889\n","\t - Precision: 0.812500\n","\t - Recall: 0.830952\n","\n","\t - Train loss: 0.0685\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.648810\n","\t - Precision: 0.687500\n","\t - Recall: 0.732143\n","\n","\t - Train loss: 0.0685\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0669\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.700877\n","\t - Precision: 0.825758\n","\t - Recall: 0.697917\n","\n","\t - Train loss: 0.0669\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0669\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0669\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.788889\n","\t - Precision: 0.812500\n","\t - Recall: 0.830952\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch:  25%|██▌       | 2/8 [01:05<03:18, 33.16s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","\t - Train loss: 0.0669\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.648810\n","\t - Precision: 0.687500\n","\t - Recall: 0.732143\n","\n","\t - Train loss: 0.0669\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.1065\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.700877\n","\t - Precision: 0.825758\n","\t - Recall: 0.697917\n","\n","\t - Train loss: 0.1065\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.1065\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.1065\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.788889\n","\t - Precision: 0.812500\n","\t - Recall: 0.830952\n","\n","\t - Train loss: 0.1065\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.648810\n","\t - Precision: 0.687500\n","\t - Recall: 0.732143\n","\n","\t - Train loss: 0.1065\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0600\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0600\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0600\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0600\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.788889\n","\t - Precision: 0.812500\n","\t - Recall: 0.830952\n","\n","\t - Train loss: 0.0600\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.648810\n","\t - Precision: 0.687500\n","\t - Recall: 0.732143\n","\n","\t - Train loss: 0.0600\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0700\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0700\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0700\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0700\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.788889\n","\t - Precision: 0.812500\n","\t - Recall: 0.830952\n","\n","\t - Train loss: 0.0700\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.648810\n","\t - Precision: 0.687500\n","\t - Recall: 0.732143\n","\n","\t - Train loss: 0.0700\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0705\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0705\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0705\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0705\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.788889\n","\t - Precision: 0.812500\n","\t - Recall: 0.830952\n","\n","\t - Train loss: 0.0705\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.701465\n","\t - Precision: 0.729167\n","\t - Recall: 0.767857\n","\n","\t - Train loss: 0.0705\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0595\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0595\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0595\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0595\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.788889\n","\t - Precision: 0.812500\n","\t - Recall: 0.830952\n","\n","\t - Train loss: 0.0595\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0595\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0512\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0512\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0512\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0512\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.655637\n","\t - Precision: 0.775000\n","\t - Recall: 0.697619\n","\n","\t - Train loss: 0.0512\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0512\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0456\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0456\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0456\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0456\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.655637\n","\t - Precision: 0.775000\n","\t - Recall: 0.697619\n","\n","\t - Train loss: 0.0456\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0456\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0499\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0499\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0499\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0499\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.655637\n","\t - Precision: 0.775000\n","\t - Recall: 0.697619\n","\n","\t - Train loss: 0.0499\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0499\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0460\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0460\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0460\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0460\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.655637\n","\t - Precision: 0.775000\n","\t - Recall: 0.697619\n","\n","\t - Train loss: 0.0460\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0460\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0424\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0424\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0424\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0424\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.655637\n","\t - Precision: 0.775000\n","\t - Recall: 0.697619\n","\n","\t - Train loss: 0.0424\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0424\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0409\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0409\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0409\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0409\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.655637\n","\t - Precision: 0.775000\n","\t - Recall: 0.697619\n","\n","\t - Train loss: 0.0409\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0409\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0433\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0433\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0433\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0433\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0433\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0433\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0410\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0410\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0410\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.566667\n","\t - Precision: 0.562500\n","\t - Recall: 0.694444\n","\n","\t - Train loss: 0.0410\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0410\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0410\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0388\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0388\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0388\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.566667\n","\t - Precision: 0.562500\n","\t - Recall: 0.694444\n","\n","\t - Train loss: 0.0388\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0388\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.830952\n","\t - Precision: 0.875000\n","\t - Recall: 0.839286\n","\n","\t - Train loss: 0.0388\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0368\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0368\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0368\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.566667\n","\t - Precision: 0.562500\n","\t - Recall: 0.694444\n","\n","\t - Train loss: 0.0368\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0368\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.830952\n","\t - Precision: 0.875000\n","\t - Recall: 0.839286\n","\n","\t - Train loss: 0.0368\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0471\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0471\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0471\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.566667\n","\t - Precision: 0.562500\n","\t - Recall: 0.694444\n","\n","\t - Train loss: 0.0471\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0471\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.830952\n","\t - Precision: 0.875000\n","\t - Recall: 0.839286\n","\n","\t - Train loss: 0.0471\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0454\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0454\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0454\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.566667\n","\t - Precision: 0.562500\n","\t - Recall: 0.694444\n","\n","\t - Train loss: 0.0454\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0454\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.830952\n","\t - Precision: 0.875000\n","\t - Recall: 0.839286\n","\n","\t - Train loss: 0.0454\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0489\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0489\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0489\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.566667\n","\t - Precision: 0.562500\n","\t - Recall: 0.694444\n","\n","\t - Train loss: 0.0489\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0489\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.830952\n","\t - Precision: 0.875000\n","\t - Recall: 0.839286\n","\n","\t - Train loss: 0.0489\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0470\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0470\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0470\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.606250\n","\t - Precision: 0.616071\n","\t - Recall: 0.722222\n","\n","\t - Train loss: 0.0470\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0470\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.830952\n","\t - Precision: 0.875000\n","\t - Recall: 0.839286\n","\n","\t - Train loss: 0.0470\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0497\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0497\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0497\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.566667\n","\t - Precision: 0.591667\n","\t - Recall: 0.694444\n","\n","\t - Train loss: 0.0497\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0497\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.830952\n","\t - Precision: 0.875000\n","\t - Recall: 0.839286\n","\n","\t - Train loss: 0.0497\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0478\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0478\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0478\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.566667\n","\t - Precision: 0.591667\n","\t - Recall: 0.694444\n","\n","\t - Train loss: 0.0478\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0478\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.830952\n","\t - Precision: 0.875000\n","\t - Recall: 0.839286\n","\n","\t - Train loss: 0.0478\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0463\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0463\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0463\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.566667\n","\t - Precision: 0.591667\n","\t - Recall: 0.694444\n","\n","\t - Train loss: 0.0463\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0463\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.830952\n","\t - Precision: 0.875000\n","\t - Recall: 0.839286\n","\n","\t - Train loss: 0.0463\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0507\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0507\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0507\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.566667\n","\t - Precision: 0.591667\n","\t - Recall: 0.694444\n","\n","\t - Train loss: 0.0507\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0507\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.830952\n","\t - Precision: 0.875000\n","\t - Recall: 0.839286\n","\n","\t - Train loss: 0.0507\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0493\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0493\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0493\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.566667\n","\t - Precision: 0.591667\n","\t - Recall: 0.694444\n","\n","\t - Train loss: 0.0493\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0493\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.830952\n","\t - Precision: 0.875000\n","\t - Recall: 0.839286\n","\n","\t - Train loss: 0.0493\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0506\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0506\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0506\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.566667\n","\t - Precision: 0.591667\n","\t - Recall: 0.694444\n","\n","\t - Train loss: 0.0506\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0506\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.830952\n","\t - Precision: 0.875000\n","\t - Recall: 0.839286\n","\n","\t - Train loss: 0.0506\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0492\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0492\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0492\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.566667\n","\t - Precision: 0.591667\n","\t - Recall: 0.694444\n","\n","\t - Train loss: 0.0492\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0492\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.830952\n","\t - Precision: 0.875000\n","\t - Recall: 0.839286\n","\n","\t - Train loss: 0.0492\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0557\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0557\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0557\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.566667\n","\t - Precision: 0.591667\n","\t - Recall: 0.694444\n","\n","\t - Train loss: 0.0557\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0557\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.830952\n","\t - Precision: 0.875000\n","\t - Recall: 0.839286\n","\n","\t - Train loss: 0.0557\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0539\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0539\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0539\n","\t - Accuracy: 0.500000\n","\t - F1 Score: 0.523810\n","\t - Precision: 0.533333\n","\t - Recall: 0.666667\n","\n","\t - Train loss: 0.0539\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0539\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.830952\n","\t - Precision: 0.875000\n","\t - Recall: 0.839286\n","\n","\t - Train loss: 0.0539\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0524\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0524\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0524\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0524\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0524\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.830952\n","\t - Precision: 0.875000\n","\t - Recall: 0.839286\n","\n","\t - Train loss: 0.0524\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0511\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0511\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0511\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0511\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0511\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0511\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0496\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0496\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0496\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0496\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0496\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0496\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0499\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0499\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0499\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0499\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0499\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.701465\n","\t - Precision: 0.729167\n","\t - Recall: 0.767857\n","\n","\t - Train loss: 0.0499\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0489\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0489\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0489\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0489\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0489\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.701465\n","\t - Precision: 0.729167\n","\t - Recall: 0.767857\n","\n","\t - Train loss: 0.0489\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0488\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0488\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0488\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0488\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0488\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.701465\n","\t - Precision: 0.729167\n","\t - Recall: 0.767857\n","\n","\t - Train loss: 0.0488\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0513\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0513\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0513\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0513\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0513\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.701465\n","\t - Precision: 0.729167\n","\t - Recall: 0.767857\n","\n","\t - Train loss: 0.0513\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0502\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0502\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0502\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0502\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0502\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.701465\n","\t - Precision: 0.729167\n","\t - Recall: 0.767857\n","\n","\t - Train loss: 0.0502\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0515\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0515\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0515\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0515\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0515\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.701465\n","\t - Precision: 0.729167\n","\t - Recall: 0.767857\n","\n","\t - Train loss: 0.0515\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0505\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0505\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0505\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0505\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0505\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.701465\n","\t - Precision: 0.729167"]},{"output_type":"stream","name":"stderr","text":["\rEpoch:  38%|███▊      | 3/8 [01:37<02:43, 32.64s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","\t - Recall: 0.767857\n","\n","\t - Train loss: 0.0505\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0472\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0472\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0472\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0472\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0472\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0472\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0267\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0267\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0267\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0267\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0267\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0267\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0367\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0367\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0367\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0367\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0367\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0367\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0299\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0299\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0299\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0299\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0299\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0299\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0255\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0255\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0255\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.635714\n","\t - Precision: 0.637500\n","\t - Recall: 0.777778\n","\n","\t - Train loss: 0.0255\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0255\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0255\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0231\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0231\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0231\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0231\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0231\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0231\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0221\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0221\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0221\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0221\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0221\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0221\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0203\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0203\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0203\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0203\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0203\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0203\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0190\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0190\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0190\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0190\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0190\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0190\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0227\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0227\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0227\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0227\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0227\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0227\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0256\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0256\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0256\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0256\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0256\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0256\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0319\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0319\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0319\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0319\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0319\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0319\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0330\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0330\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0330\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0330\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0330\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0330\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0311\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0311\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0311\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0311\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0311\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0311\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0295\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0295\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0295\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0295\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0295\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0295\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0281\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0281\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0281\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0281\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0281\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0281\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0289\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0289\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0289\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0289\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0289\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0289\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0277\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0277\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0277\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0277\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.701389\n","\t - Precision: 0.791667\n","\t - Recall: 0.747619\n","\n","\t - Train loss: 0.0277\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0277\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0267\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0267\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0267\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0267\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.701389\n","\t - Precision: 0.791667\n","\t - Recall: 0.747619\n","\n","\t - Train loss: 0.0267\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0267\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0274\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0274\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0274\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0274\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.701389\n","\t - Precision: 0.791667\n","\t - Recall: 0.747619\n","\n","\t - Train loss: 0.0274\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0274\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0297\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0297\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0297\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0297\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.701389\n","\t - Precision: 0.791667\n","\t - Recall: 0.747619\n","\n","\t - Train loss: 0.0297\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0297\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0287\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0287\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0287\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0287\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.701389\n","\t - Precision: 0.791667\n","\t - Recall: 0.747619\n","\n","\t - Train loss: 0.0287\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0287\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0278\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0278\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0278\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0278\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.701389\n","\t - Precision: 0.791667\n","\t - Recall: 0.747619\n","\n","\t - Train loss: 0.0278\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0278\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0270\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0270\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0270\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0270\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.701389\n","\t - Precision: 0.791667\n","\t - Recall: 0.747619\n","\n","\t - Train loss: 0.0270\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0270\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0261\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0261\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0261\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0261\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.701389\n","\t - Precision: 0.791667\n","\t - Recall: 0.747619\n","\n","\t - Train loss: 0.0261\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0261\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0254\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0254\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0254\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0254\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.701389\n","\t - Precision: 0.791667\n","\t - Recall: 0.747619\n","\n","\t - Train loss: 0.0254\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0254\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0273\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0273\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0273\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0273\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.701389\n","\t - Precision: 0.791667\n","\t - Recall: 0.747619\n","\n","\t - Train loss: 0.0273\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0273\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0266\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0266\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0266\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0266\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.701389\n","\t - Precision: 0.791667\n","\t - Recall: 0.747619\n","\n","\t - Train loss: 0.0266\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0266\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0281\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0281\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0281\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0281\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.701389\n","\t - Precision: 0.791667\n","\t - Recall: 0.747619\n","\n","\t - Train loss: 0.0281\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0281\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0335\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0335\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0335\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0335\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0335\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0335\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0331\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0331\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0331\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0331\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0331\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0331\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0323\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0323\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0323\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0323\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0323\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0323\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0315\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0315\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0315\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0315\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0315\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0315\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0373\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0373\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0373\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.684524\n","\t - Precision: 0.666667\n","\t - Recall: 0.805556\n","\n","\t - Train loss: 0.0373\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0373\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0373\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0364\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0364\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0364\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.684524\n","\t - Precision: 0.666667\n","\t - Recall: 0.805556\n","\n","\t - Train loss: 0.0364\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0364\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0364\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0389\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0389\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0389\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.684524\n","\t - Precision: 0.666667\n","\t - Recall: 0.805556\n","\n","\t - Train loss: 0.0389\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0389\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0389\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0396\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0396\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0396\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.684524\n","\t - Precision: 0.666667\n","\t - Recall: 0.805556\n","\n","\t - Train loss: 0.0396\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0396\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0396\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0389\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0389\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0389\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.684524\n","\t - Precision: 0.666667\n","\t - Recall: 0.805556\n","\n","\t - Train loss: 0.0389\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch:  50%|█████     | 4/8 [02:08<02:07, 31.79s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","\t - Train loss: 0.0389\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0389\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.2058\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.2058\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.2058\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.635714\n","\t - Precision: 0.637500\n","\t - Recall: 0.777778\n","\n","\t - Train loss: 0.2058\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.2058\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.2058\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.1067\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.1067\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.1067\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.1067\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.1067\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.1067\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0731\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0731\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0731\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0731\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0731\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0731\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0739\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0739\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0739\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0739\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0739\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0739\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0885\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0885\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0885\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0885\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0885\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0885\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0755\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0755\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0755\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0755\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0755\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.0755\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0656\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0656\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0656\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0656\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0656\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.0656\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0627\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0627\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0627\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0627\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0627\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.0627\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0564\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0564\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0564\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0564\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0564\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.0564\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0513\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0513\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0513\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0513\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0513\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.0513\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0528\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0528\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0528\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0528\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0528\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.0528\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0491\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0491\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0491\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0491\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0491\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.0491\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0458\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0458\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0458\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0458\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0458\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.0458\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0470\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0470\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0470\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0470\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0470\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.0470\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0470\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0470\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0470\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0470\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0470\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.0470\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0476\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0476\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0476\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0476\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0476\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.0476\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0504\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0504\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0504\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0504\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0504\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.0504\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0479\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0479\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0479\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0479\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0479\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.0479\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0458\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0458\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0458\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0458\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0458\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.0458\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0438\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0438\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0438\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0438\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0438\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.0438\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0421\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0421\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0421\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0421\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0421\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.0421\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0405\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0405\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0405\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0405\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0405\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.0405\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0403\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0403\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0403\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0403\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.701389\n","\t - Precision: 0.791667\n","\t - Recall: 0.747619\n","\n","\t - Train loss: 0.0403\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.0403\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0388\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0388\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0388\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0388\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.701389\n","\t - Precision: 0.791667\n","\t - Recall: 0.747619\n","\n","\t - Train loss: 0.0388\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.0388\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0376\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0376\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0376\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0376\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.701389\n","\t - Precision: 0.791667\n","\t - Recall: 0.747619\n","\n","\t - Train loss: 0.0376\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.0376\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0364\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0364\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0364\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0364\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.701389\n","\t - Precision: 0.791667\n","\t - Recall: 0.747619\n","\n","\t - Train loss: 0.0364\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.0364\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0352\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0352\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0352\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0352\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.701389\n","\t - Precision: 0.791667\n","\t - Recall: 0.747619\n","\n","\t - Train loss: 0.0352\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.0352\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0342\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0342\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0342\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0342\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.701389\n","\t - Precision: 0.791667\n","\t - Recall: 0.747619\n","\n","\t - Train loss: 0.0342\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.0342\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0332\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0332\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0332\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0332\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.701389\n","\t - Precision: 0.791667\n","\t - Recall: 0.747619\n","\n","\t - Train loss: 0.0332\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.0332\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0323\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0323\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0323\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0323\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.701389\n","\t - Precision: 0.791667\n","\t - Recall: 0.747619\n","\n","\t - Train loss: 0.0323\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.0323\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0355\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0355\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0355\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0355\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.701389\n","\t - Precision: 0.791667\n","\t - Recall: 0.747619\n","\n","\t - Train loss: 0.0355\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.820879\n","\t - Precision: 0.812500\n","\t - Recall: 0.866071\n","\n","\t - Train loss: 0.0355\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0401\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0401\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0401\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0401\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0401\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0401\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0391\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0391\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0391\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0391\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0391\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0391\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0381\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0381\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0381\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0381\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0381\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0381\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0371\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0371\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0371\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0371\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0371\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0371\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0394\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0394\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0394\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.635714\n","\t - Precision: 0.637500\n","\t - Recall: 0.777778\n","\n","\t - Train loss: 0.0394\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0394\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0394\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0385\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0385\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0385\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.635714\n","\t - Precision: 0.637500\n","\t - Recall: 0.777778\n","\n","\t - Train loss: 0.0385\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0385\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0385\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0376\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0376\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0376\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.684524\n","\t - Precision: 0.666667\n","\t - Recall: 0.805556\n","\n","\t - Train loss: 0.0376\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0376\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch:  62%|██████▎   | 5/8 [02:40<01:35, 31.89s/it]"]},{"output_type":"stream","name":"stdout","text":["\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0376\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0382\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0382\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0382\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0382\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0382\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0382\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0216\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0216\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0216\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0216\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0216\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0216\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0170\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0170\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0170\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0170\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0170\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0170\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0138\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0138\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0138\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0138\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0138\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0138\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0238\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0238\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0238\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0238\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0238\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0238\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0207\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0207\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0207\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0207\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0207\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0207\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0274\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0274\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0274\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0274\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0274\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0274\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0344\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0344\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0344\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0344\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0344\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0344\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0311\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0311\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0311\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0311\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0311\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0311\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0285\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0285\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0285\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0285\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0285\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0285\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0264\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0264\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0264\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0264\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0264\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0264\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0246\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0246\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0246\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0246\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0246\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0246\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0231\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0231\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0231\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0231\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0231\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0231\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0393\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0393\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0393\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0393\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0393\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0393\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0370\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0370\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0370\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0370\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0370\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0370\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0391\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0391\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0391\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.747024\n","\t - Precision: 0.714286\n","\t - Recall: 0.833333\n","\n","\t - Train loss: 0.0391\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0391\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0391\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0389\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0389\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0389\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.684524\n","\t - Precision: 0.666667\n","\t - Recall: 0.805556\n","\n","\t - Train loss: 0.0389\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0389\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0389\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0406\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0406\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0406\n","\t - Accuracy: 0.687500\n","\t - F1 Score: 0.684524\n","\t - Precision: 0.666667\n","\t - Recall: 0.805556\n","\n","\t - Train loss: 0.0406\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0406\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0406\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0449\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0449\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0449\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.635714\n","\t - Precision: 0.637500\n","\t - Recall: 0.777778\n","\n","\t - Train loss: 0.0449\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0449\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0449\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0459\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0459\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0459\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0459\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0459\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0459\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0440\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0440\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0440\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0440\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0440\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0440\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0521\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0521\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0521\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0521\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0521\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0521\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0501\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0501\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0501\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0501\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0501\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0501\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0508\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0508\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0508\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0508\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0508\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0508\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0490\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0490\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0490\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0490\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0490\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0490\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0474\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0474\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0474\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0474\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0474\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0474\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0458\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0458\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0458\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0458\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0458\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0458\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0444\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0444\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0444\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0444\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0444\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0444\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0431\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0431\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0431\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0431\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0431\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0431\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0419\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0419\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0419\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0419\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0419\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0419\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0407\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0407\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0407\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0407\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0407\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0407\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0396\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0396\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0396\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0396\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0396\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0396\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0386\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0386\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0386\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0386\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0386\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0386\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0376\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0376\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0376\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0376\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0376\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0376\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0404\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0404\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0404\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0404\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0404\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0404\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0395\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0395\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0395\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0395\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0395\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0395\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0411\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0411\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0411\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0411\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0411\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0411\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0401\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0401\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0401\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0401\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch:  75%|███████▌  | 6/8 [03:16<01:06, 33.31s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","\t - Train loss: 0.0401\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0401\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0979\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0979\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0979\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0979\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0979\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0979\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0783\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0783\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0783\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0783\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0783\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0783\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0538\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0538\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0538\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0538\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0538\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0538\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0526\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0526\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0526\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0526\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0526\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0526\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0533\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0533\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0533\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0533\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0533\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0533\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0538\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0538\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0538\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0538\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0538\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0538\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0468\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0468\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0468\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0468\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0468\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0468\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0462\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0462\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0462\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0462\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0462\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0462\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0479\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0479\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0479\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0479\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0479\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0479\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0474\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0474\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0474\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0474\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0474\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0474\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0435\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0435\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0435\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0435\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0435\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0435\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0403\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0403\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0403\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0403\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0403\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0403\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0380\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0380\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0380\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0380\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0380\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0380\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0356\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0356\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0356\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0356\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0356\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0356\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0336\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0336\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0336\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0336\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0336\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0336\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0356\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0356\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0356\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0356\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0356\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0356\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0339\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0339\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0339\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0339\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0339\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0339\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0323\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0323\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0323\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0323\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0323\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0323\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0338\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0338\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0338\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0338\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0338\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0338\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0323\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0323\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0323\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0323\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0323\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0323\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0325\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0325\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0325\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0325\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0325\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0325\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0328\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0328\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0328\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0328\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0328\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0328\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0336\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0336\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0336\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0336\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0336\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0336\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0323\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0323\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0323\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0323\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0323\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0323\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0312\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0312\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0312\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0312\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0312\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0312\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0302\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0302\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0302\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0302\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0302\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0302\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0292\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0292\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0292\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0292\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0292\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0292\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0349\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0349\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0349\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0349\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0349\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0349\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0338\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0338\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0338\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0338\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0338\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0338\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0328\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0328\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0328\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0328\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0328\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0328\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0319\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0319\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0319\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0319\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0319\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0319\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0310\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0310\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0310\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0310\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0310\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0310\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0302\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0302\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0302\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0302\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0302\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0302\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0313\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0313\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0313\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0313\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0313\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0313\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0323\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0323\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0323\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0323\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0323\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0323\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0315\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0315\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0315\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0315\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0315\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0315\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0348\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0348\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0348\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0348\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0348\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0348\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0340\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0340\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0340\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0340\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0340\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch:  88%|████████▊ | 7/8 [03:48<00:32, 32.96s/it]"]},{"output_type":"stream","name":"stdout","text":["\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0340\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0724\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0724\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0724\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0724\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0724\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0724\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0610\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0610\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0610\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0610\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0610\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0610\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0421\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0421\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0421\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0421\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0421\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0421\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0325\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0325\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0325\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0325\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0325\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0325\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0348\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0348\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0348\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0348\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0348\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0348\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0298\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0298\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0298\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0298\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0298\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0298\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0261\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0261\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0261\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0261\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0261\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0261\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0293\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0293\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0293\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0293\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0293\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0293\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0265\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0265\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0265\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0265\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0265\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0265\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0293\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0293\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0293\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0293\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0293\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0293\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0312\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0312\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0312\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0312\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0312\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0312\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0323\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0323\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0323\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0323\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0323\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0323\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0307\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0307\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0307\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0307\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0307\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0307\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0288\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0288\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0288\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0288\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0288\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0288\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0303\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0303\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0303\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0303\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0303\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0303\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0287\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0287\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0287\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0287\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0287\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0287\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0272\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0272\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0272\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0272\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0272\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0272\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0293\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0293\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0293\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0293\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0293\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0293\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0279\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0279\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0279\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0279\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0279\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0279\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0267\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0267\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0267\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0267\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0267\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0267\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0256\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0256\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0256\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0256\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0256\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0256\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0246\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0246\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0246\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0246\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0246\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0246\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0239\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0239\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0239\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0239\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0239\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0239\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0230\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0230\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0230\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0230\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0230\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0230\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0222\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0222\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0222\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0222\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0222\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0222\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0215\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0215\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0215\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0215\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0215\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0215\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0209\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0209\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0209\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0209\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0209\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0209\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0203\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0203\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0203\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0203\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.756944\n","\t - Precision: 0.909091\n","\t - Recall: 0.733333\n","\n","\t - Train loss: 0.0203\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0203\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0229\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0229\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0229\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0229\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0229\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0229\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0235\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0235\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0235\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0235\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0235\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0235\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0246\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0246\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0246\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0246\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0246\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0246\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0240\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0240\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0240\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0240\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0240\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0240\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0301\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0301\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0301\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0301\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0301\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0301\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0294\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0294\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0294\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0294\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0294\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0294\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0302\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0302\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0302\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0302\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0302\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0302\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0322\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0322\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0322\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0322\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0322\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0322\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0314\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0314\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0314\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0314\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n","\n","\t - Train loss: 0.0314\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0314\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n","\n","\t - Train loss: 0.0307\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.625000\n","\t - Precision: 0.770833\n","\t - Recall: 0.614583\n","\n","\t - Train loss: 0.0307\n","\t - Accuracy: 0.625000\n","\t - F1 Score: 0.500000\n","\t - Precision: 0.462500\n","\t - Recall: 0.587500\n","\n","\t - Train loss: 0.0307\n","\t - Accuracy: 0.562500\n","\t - F1 Score: 0.591575\n","\t - Precision: 0.612500\n","\t - Recall: 0.750000\n","\n","\t - Train loss: 0.0307\n","\t - Accuracy: 0.812500\n","\t - F1 Score: 0.803105\n","\t - Precision: 0.925000\n","\t - Recall: 0.783333\n"]},{"output_type":"stream","name":"stderr","text":["Epoch: 100%|██████████| 8/8 [04:21<00:00, 32.69s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","\t - Train loss: 0.0307\n","\t - Accuracy: 0.750000\n","\t - F1 Score: 0.759524\n","\t - Precision: 0.782738\n","\t - Recall: 0.803571\n","\n","\t - Train loss: 0.0307\n","\t - Accuracy: 0.777778\n","\t - F1 Score: 0.637500\n","\t - Precision: 0.687500\n","\t - Recall: 0.604167\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","epochs = 8\n","\n","for _ in trange (epochs, desc = 'Epoch'):\n","  model.train()\n","  tr_loss = 0\n","  nb_tr_examples, nb_tr_steps = 0, 0\n","\n","  for step, batch in enumerate(train_dataloader):\n","      batch = tuple(t.to(device) for t in batch)\n","      b_input_ids, b_input_mask, b_labels = batch\n","      #reset gradient value for the new epoch\n","      optimizer.zero_grad()\n","      # Forward pass\n","      train_output = model(b_input_ids, \n","                            token_type_ids = None, \n","                            attention_mask = b_input_mask, \n","                            labels = b_labels)\n","      # Backward pass\n","      train_output.loss.backward()\n","      optimizer.step()\n","      # Update tracking variables\n","      tr_loss += train_output.loss.item()\n","      nb_tr_examples += b_input_ids.size(0)\n","      nb_tr_steps += 1\n","\n","       # ========== Validation ==========\n","\n","      # Set model to evaluation mode\n","      model.eval()\n","\n","      # Tracking variables \n","      val_accuracy = []\n","      val_precision = []\n","      val_recall = []\n","      val_specificity = []\n","\n","      latest_acc = 0.0000;\n","\n","      for batch in validation_dataloader:\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","        with torch.no_grad():\n","          # Forward pass\n","          eval_output = model(b_input_ids, \n","                              token_type_ids = None, \n","                              attention_mask = b_input_mask)\n","        logits = eval_output.logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        # Calculate validation metrics for the entire model\n","        predictions = np.argmax(logits, axis=1)\n","        print('\\n\\t - Train loss: {:.5f}'.format(tr_loss / nb_tr_steps))\n","        print('\\t - Accuracy: {:.4f}'.format(accuracy.compute(predictions=predictions, references=label_ids)['accuracy']))\n","        print('\\t - F1 Score: {:.4f}'.format(f1.compute(predictions=predictions, references=label_ids, average=\"macro\")['f1']))\n","        print('\\t - Precision: {:.4f}'.format(precision.compute(predictions=predictions, references=label_ids, average=\"macro\", zero_division=0)['precision']))\n","        print('\\t - Recall: {:.4f}'.format(recall.compute(predictions=predictions, references=label_ids, average=\"macro\", zero_division=0)['recall']))\n","        print('\\t - Recall: {:.4f}'.format(mcc.compute(predictions=predictions, references=label_ids, average=\"macro\", zero_division=0)['matthews_correlation']))\n","\n","\n","PATH = './greet'\n","torch.save(model, PATH)"]},{"cell_type":"markdown","source":["# Load the Fine Tuned Model"],"metadata":{"id":"MHILlUkYr3MK"}},{"cell_type":"code","execution_count":21,"metadata":{"id":"AjDipcdWy5Bv","executionInfo":{"status":"ok","timestamp":1685431206021,"user_tz":-120,"elapsed":377,"user":{"displayName":"GIOVANNI SCORZIELLO","userId":"08861605982515496649"}}},"outputs":[],"source":["# load the model saved\n","PATH = './greet';\n","model = torch.load(PATH)"]},{"cell_type":"markdown","source":["# Model Testing"],"metadata":{"id":"QgUpFQSwr82G"}},{"cell_type":"markdown","source":["Now let's go test our model on the portion of the Dataset that has been specially split for this purpose, in order to train our model on data it has never seen, again here we are going to print the model metrics for testing"],"metadata":{"id":"dgjCAj48sB5M"}},{"cell_type":"code","source":["for batch in test_dataloader:\n","    batch = tuple(t.to(device) for t in batch)\n","    b_input_ids, b_input_mask, b_labels = batch\n","\n","    eval_output = model(b_input_ids, \n","                        token_type_ids = None, \n","                        attention_mask = b_input_mask)\n","    logits = eval_output.logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","    # Calculate validation metrics for each class\n","    b_accuracy_mop, b_precision_mop, b_recall_mop, b_specificity_mop = b_metrics(logits, label_ids, 0)\n","    b_accuracy_aop, b_precision_aop, b_recall_aop, b_specificity_aop = b_metrics(logits, label_ids, 1)\n","    b_accuracy_clr, b_precision_clr, b_recall_clr, b_specificity_clr = b_metrics(logits, label_ids, 2)\n","    b_accuracy_nic, b_precision_nic, b_recall_nic, b_specificity_nic = b_metrics(logits, label_ids, 3)\n","    # for the entire model\n","    test_accuracy, test_precision, test_recall, test_specificity = total_metrics(logits, label_ids)\n","\n","    test_predictions = np.argmax(logits, axis=1)\n","    print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n","    print('\\t - Accuracy: {:4f}'.format(accuracy.compute(predictions=test_predictions, references=label_ids)['accuracy']))\n","    print('\\t - F1 Score: {:4f}'.format(f1.compute(predictions=test_predictions, references=label_ids, average=\"macro\")['f1']))\n","    print('\\t - Precision: {:4f}'.format(precision.compute(predictions=test_predictions, references=label_ids, average=\"macro\")['precision']))\n","    print('\\t - Recall: {:4f}'.format(recall.compute(predictions=test_predictions, references=label_ids, average=\"macro\")['recall']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MKHIcj5BxrWL","executionInfo":{"status":"ok","timestamp":1685431210946,"user_tz":-120,"elapsed":1337,"user":{"displayName":"GIOVANNI SCORZIELLO","userId":"08861605982515496649"}},"outputId":"943016ec-bd4a-40c9-99f3-2a2f49353c76"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================MOP=============================================================\n","\n","\t - Validation Accuracy: 0.7500\n","\t - Validation Precision: 1.0000\n","\t - Validation Recall: 0.5000\n","\t - Validation Specificity: 1.0000\n","\n","======================================================AOP=============================================================\n","\n","\t - Validation Accuracy: 0.7500\n","\t - Validation Precision: 1.0000\n","\t - Validation Recall: 0.5000\n","\t - Validation Specificity: 1.0000\n","\n","======================================================NIC=============================================================\n","\n","\t - Validation Accuracy: 0.7500\n","\t - Validation Precision: 1.0000\n","\t - Validation Recall: 0.3333\n","\t - Validation Specificity: 1.0000\n","\n","======================================================CLR=============================================================\n","\n","\t - Validation Accuracy: 0.7500\n","\t - Validation Precision: 0.6000\n","\t - Validation Recall: 0.6000\n","\t - Validation Specificity: 0.6000\n","\n","======================================================TOT=============================================================\n","\n","\n","\t - Train loss: 0.3010\n","\t - Validation Accuracy: 0.8571\n","\t - Validation Precision: 0.7500\n","\t - Validation Recall: 0.7500\n","\t - Validation Specificity: 0.9000\n","\n","======================================================MOP=============================================================\n","\n","\t - Validation Accuracy: 0.6875\n","\t - Validation Precision: nan\n","\t - Validation Recall: 0.0000\n","\t - Validation Specificity: 1.0000\n","\n","======================================================AOP=============================================================\n","\n","\t - Validation Accuracy: 0.6875\n","\t - Validation Precision: 1.0000\n","\t - Validation Recall: 0.4286\n","\t - Validation Specificity: 1.0000\n","\n","======================================================NIC=============================================================\n","\n","\t - Validation Accuracy: 0.6875\n","\t - Validation Precision: nan\n","\t - Validation Recall: 0.0000\n","\t - Validation Specificity: 1.0000\n","\n","======================================================CLR=============================================================\n","\n","\t - Validation Accuracy: 0.6875\n","\t - Validation Precision: 0.6154\n","\t - Validation Recall: 0.6154\n","\t - Validation Specificity: 0.3750\n","\n","======================================================TOT=============================================================\n","\n","\n","\t - Train loss: 0.3010\n","\t - Validation Accuracy: 0.8148\n","\t - Validation Precision: 0.6875\n","\t - Validation Recall: 0.6875\n","\t - Validation Specificity: 0.8684\n","\n","======================================================MOP=============================================================\n","\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-11-38567bcd30bc>:9: RuntimeWarning: invalid value encountered in long_scalars\n","  b_precision = tp / (tp + fp) #if (tp + fp) > 0 else 'nan'\n"]},{"output_type":"stream","name":"stdout","text":["\t - Validation Accuracy: 0.6250\n","\t - Validation Precision: nan\n","\t - Validation Recall: 0.0000\n","\t - Validation Specificity: 1.0000\n","\n","======================================================AOP=============================================================\n","\n","\t - Validation Accuracy: 0.6250\n","\t - Validation Precision: 0.0000\n","\t - Validation Recall: 0.0000\n","\t - Validation Specificity: 0.9091\n","\n","======================================================NIC=============================================================\n","\n","\t - Validation Accuracy: 0.6250\n","\t - Validation Precision: 1.0000\n","\t - Validation Recall: 0.2500\n","\t - Validation Specificity: 1.0000\n","\n","======================================================CLR=============================================================\n","\n","\t - Validation Accuracy: 0.6250\n","\t - Validation Precision: 0.6154\n","\t - Validation Recall: 0.6154\n","\t - Validation Specificity: 0.2857\n","\n","======================================================TOT=============================================================\n","\n","\n","\t - Train loss: 0.3010\n","\t - Validation Accuracy: 0.7692\n","\t - Validation Precision: 0.6250\n","\t - Validation Recall: 0.6250\n","\t - Validation Specificity: 0.8333\n","\n","======================================================MOP=============================================================\n","\n","\t - Validation Accuracy: 0.5000\n","\t - Validation Precision: 0.0000\n","\t - Validation Recall: 0.0000\n","\t - Validation Specificity: 0.8889\n","\n","======================================================AOP=============================================================\n","\n","\t - Validation Accuracy: 0.5000\n","\t - Validation Precision: nan\n","\t - Validation Recall: 0.0000\n","\t - Validation Specificity: 1.0000\n","\n","======================================================NIC=============================================================\n","\n","\t - Validation Accuracy: 0.5000\n","\t - Validation Precision: nan\n","\t - Validation Recall: 0.0000\n","\t - Validation Specificity: 1.0000\n","\n","======================================================CLR=============================================================\n","\n","\t - Validation Accuracy: 0.5000\n","\t - Validation Precision: 0.5333\n","\t - Validation Recall: 0.5333\n","\t - Validation Specificity: 0.0000\n","\n","======================================================TOT=============================================================\n","\n","\n","\t - Train loss: 0.3010\n","\t - Validation Accuracy: 0.6667\n","\t - Validation Precision: 0.5000\n","\t - Validation Recall: 0.5000\n","\t - Validation Specificity: 0.7500\n","\n","======================================================MOP=============================================================\n","\n","\t - Validation Accuracy: 0.6875\n","\t - Validation Precision: 0.3333\n","\t - Validation Recall: 0.2000\n","\t - Validation Specificity: 0.8333\n","\n","======================================================AOP=============================================================\n","\n","\t - Validation Accuracy: 0.6875\n","\t - Validation Precision: 0.5000\n","\t - Validation Recall: 0.1667\n","\t - Validation Specificity: 0.9091\n","\n","======================================================NIC=============================================================\n","\n","\t - Validation Accuracy: 0.6875\n","\t - Validation Precision: 1.0000\n","\t - Validation Recall: 0.3333\n","\t - Validation Specificity: 1.0000\n","\n","======================================================CLR=============================================================\n","\n","\t - Validation Accuracy: 0.6875\n","\t - Validation Precision: 0.7778\n","\t - Validation Recall: 0.7778\n","\t - Validation Specificity: 0.6667\n","\n","======================================================TOT=============================================================\n","\n","\n","\t - Train loss: 0.3010\n","\t - Validation Accuracy: 0.8148\n","\t - Validation Precision: 0.6875\n","\t - Validation Recall: 0.6875\n","\t - Validation Specificity: 0.8684\n","\n","======================================================MOP=============================================================\n","\n","\t - Validation Accuracy: 0.5625\n","\t - Validation Precision: 0.0000\n","\t - Validation Recall: 0.0000\n","\t - Validation Specificity: 0.9000\n","\n","======================================================AOP=============================================================\n","\n","\t - Validation Accuracy: 0.5625\n","\t - Validation Precision: 0.3333\n","\t - Validation Recall: 0.1429\n","\t - Validation Specificity: 0.8000\n","\n","======================================================NIC=============================================================\n","\n","\t - Validation Accuracy: 0.5625\n","\t - Validation Precision: 0.5000\n","\t - Validation Recall: 0.1250\n","\t - Validation Specificity: 0.8889\n","\n","======================================================CLR=============================================================\n","\n","\t - Validation Accuracy: 0.5625\n","\t - Validation Precision: 0.7000\n","\t - Validation Recall: 0.7000\n","\t - Validation Specificity: 0.4000\n","\n","======================================================TOT=============================================================\n","\n","\n","\t - Train loss: 0.3010\n","\t - Validation Accuracy: 0.7200\n","\t - Validation Precision: 0.5625\n","\t - Validation Recall: 0.5625\n","\t - Validation Specificity: 0.7941\n","\n","======================================================MOP=============================================================\n","\n","\t - Validation Accuracy: 0.6875\n","\t - Validation Precision: 1.0000\n","\t - Validation Recall: 0.3333\n","\t - Validation Specificity: 1.0000\n","\n","======================================================AOP=============================================================\n","\n","\t - Validation Accuracy: 0.6875\n","\t - Validation Precision: 1.0000\n","\t - Validation Recall: 0.4000\n","\t - Validation Specificity: 1.0000\n","\n","======================================================NIC=============================================================\n","\n","\t - Validation Accuracy: 0.6875\n","\t - Validation Precision: 1.0000\n","\t - Validation Recall: 0.1667\n","\t - Validation Specificity: 1.0000\n","\n","======================================================CLR=============================================================\n","\n","\t - Validation Accuracy: 0.6875\n","\t - Validation Precision: 0.5833\n","\t - Validation Recall: 0.5833\n","\t - Validation Specificity: 0.4444\n","\n","======================================================TOT=============================================================\n","\n","\n","\t - Train loss: 0.3010\n","\t - Validation Accuracy: 0.8148\n","\t - Validation Precision: 0.6875\n","\t - Validation Recall: 0.6875\n","\t - Validation Specificity: 0.8684\n","\n","======================================================MOP=============================================================\n","\n","\t - Validation Accuracy: 0.8750\n","\t - Validation Precision: 1.0000\n","\t - Validation Recall: 0.5000\n","\t - Validation Specificity: 1.0000\n","\n","======================================================AOP=============================================================\n","\n","\t - Validation Accuracy: 0.8750\n","\t - Validation Precision: 1.0000\n","\t - Validation Recall: 0.5000\n","\t - Validation Specificity: 1.0000\n","\n","======================================================NIC=============================================================\n","\n","\t - Validation Accuracy: 0.8750\n","\t - Validation Precision: 1.0000\n","\t - Validation Recall: 0.5000\n","\t - Validation Specificity: 1.0000\n","\n","======================================================CLR=============================================================\n","\n","\t - Validation Accuracy: 0.8750\n","\t - Validation Precision: 0.8333\n","\t - Validation Recall: 0.8333\n","\t - Validation Specificity: 0.6667\n","\n","======================================================TOT=============================================================\n","\n","\n","\t - Train loss: 0.3010\n","\t - Validation Accuracy: 0.9333\n","\t - Validation Precision: 0.8750\n","\t - Validation Recall: 0.8750\n","\t - Validation Specificity: 0.9545\n","\n","======================================================MOP=============================================================\n","\n","\t - Validation Accuracy: 0.7500\n","\t - Validation Precision: nan\n","\t - Validation Recall: 0.0000\n","\t - Validation Specificity: 1.0000\n","\n","======================================================AOP=============================================================\n","\n","\t - Validation Accuracy: 0.7500\n","\t - Validation Precision: 1.0000\n","\t - Validation Recall: 0.3333\n","\t - Validation Specificity: 1.0000\n","\n","======================================================NIC=============================================================\n","\n","\t - Validation Accuracy: 0.7500\n","\t - Validation Precision: 1.0000\n","\t - Validation Recall: 0.4000\n","\t - Validation Specificity: 1.0000\n","\n","======================================================CLR=============================================================\n","\n","\t - Validation Accuracy: 0.7500\n","\t - Validation Precision: 0.6667\n","\t - Validation Recall: 0.6667\n","\t - Validation Specificity: 0.5000\n","\n","======================================================TOT=============================================================\n","\n","\n","\t - Train loss: 0.3010\n","\t - Validation Accuracy: 0.8571\n","\t - Validation Precision: 0.7500\n","\t - Validation Recall: 0.7500\n","\t - Validation Specificity: 0.9000\n","\n","======================================================MOP=============================================================\n","\n","\t - Validation Accuracy: 0.6250\n","\t - Validation Precision: nan\n","\t - Validation Recall: 0.0000\n","\t - Validation Specificity: 1.0000\n","\n","======================================================AOP=============================================================\n","\n","\t - Validation Accuracy: 0.6250\n","\t - Validation Precision: 1.0000\n","\t - Validation Recall: 0.1667\n","\t - Validation Specificity: 1.0000\n","\n","======================================================NIC=============================================================\n","\n","\t - Validation Accuracy: 0.6250\n","\t - Validation Precision: 1.0000\n","\t - Validation Recall: 0.2500\n","\t - Validation Specificity: 1.0000\n","\n","======================================================CLR=============================================================\n","\n","\t - Validation Accuracy: 0.6250\n","\t - Validation Precision: 0.5385\n","\t - Validation Recall: 0.5385\n","\t - Validation Specificity: 0.3333\n","\n","======================================================TOT=============================================================\n","\n","\n","\t - Train loss: 0.3010\n","\t - Validation Accuracy: 0.7692\n","\t - Validation Precision: 0.6250\n","\t - Validation Recall: 0.6250\n","\t - Validation Specificity: 0.8333\n","\n","======================================================MOP=============================================================\n","\n","\t - Validation Accuracy: 0.6875\n","\t - Validation Precision: nan\n","\t - Validation Recall: 0.0000\n","\t - Validation Specificity: 1.0000\n","\n","======================================================AOP=============================================================\n","\n","\t - Validation Accuracy: 0.6875\n","\t - Validation Precision: 1.0000\n","\t - Validation Recall: 0.4286\n","\t - Validation Specificity: 1.0000\n","\n","======================================================NIC=============================================================\n","\n","\t - Validation Accuracy: 0.6875\n","\t - Validation Precision: 1.0000\n","\t - Validation Recall: 0.2857\n","\t - Validation Specificity: 1.0000\n","\n","======================================================CLR=============================================================\n","\n","\t - Validation Accuracy: 0.6875\n","\t - Validation Precision: 0.5455\n","\t - Validation Recall: 0.5455\n","\t - Validation Specificity: 0.5000\n","\n","======================================================TOT=============================================================\n","\n","\n","\t - Train loss: 0.3010\n","\t - Validation Accuracy: 0.8148\n","\t - Validation Precision: 0.6875\n","\t - Validation Recall: 0.6875\n","\t - Validation Specificity: 0.8684\n","\n","======================================================MOP=============================================================\n","\n","\t - Validation Accuracy: 0.7500\n","\t - Validation Precision: nan\n","\t - Validation Recall: 0.0000\n","\t - Validation Specificity: 1.0000\n","\n","======================================================AOP=============================================================\n","\n","\t - Validation Accuracy: 0.7500\n","\t - Validation Precision: 1.0000\n","\t - Validation Recall: 0.5000\n","\t - Validation Specificity: 1.0000\n","\n","======================================================NIC=============================================================\n","\n","\t - Validation Accuracy: 0.7500\n","\t - Validation Precision: 1.0000\n","\t - Validation Recall: 0.5000\n","\t - Validation Specificity: 1.0000\n","\n","======================================================CLR=============================================================\n","\n","\t - Validation Accuracy: 0.7500\n","\t - Validation Precision: 0.6000\n","\t - Validation Recall: 0.6000\n","\t - Validation Specificity: 0.6000\n","\n","======================================================TOT=============================================================\n","\n","\n","\t - Train loss: 0.3010\n","\t - Validation Accuracy: 0.8571\n","\t - Validation Precision: 0.7500\n","\t - Validation Recall: 0.7500\n","\t - Validation Specificity: 0.9000\n","\n","======================================================MOP=============================================================\n","\n","\t - Validation Accuracy: 0.6667\n","\t - Validation Precision: nan\n","\t - Validation Recall: 0.0000\n","\t - Validation Specificity: 1.0000\n","\n","======================================================AOP=============================================================\n","\n","\t - Validation Accuracy: 0.6667\n","\t - Validation Precision: 0.8000\n","\t - Validation Recall: 0.4444\n","\t - Validation Specificity: 0.8571\n","\n","======================================================NIC=============================================================\n","\n","\t - Validation Accuracy: 0.6667\n","\t - Validation Precision: nan\n","\t - Validation Recall: 0.0000\n","\t - Validation Specificity: 1.0000\n","\n","======================================================CLR=============================================================\n","\n","\t - Validation Accuracy: 0.6667\n","\t - Validation Precision: 0.6000\n","\t - Validation Recall: 0.6000\n","\t - Validation Specificity: 0.5000\n","\n","======================================================TOT=============================================================\n","\n","\n","\t - Train loss: 0.3010\n","\t - Validation Accuracy: 0.8000\n","\t - Validation Precision: 0.6667\n","\t - Validation Recall: 0.6667\n","\t - Validation Specificity: 0.8571\n","\n"]}]},{"cell_type":"markdown","source":["We rerun the testing on the same portion of data as before, but this time the data on which the model has made the prediction are printed on the screen, plus the prediction and the oracle for each data, so as to be able to verify on which portions of code the model has more problems and improve the dataset, so as to improve the performance of the model as well"],"metadata":{"id":"QoU1LlK5h44t"}},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e_D1MG6fbeNe","outputId":"d699c938-7016-4934-a092-83a5644045da","executionInfo":{"status":"ok","timestamp":1685431240670,"user_tz":-120,"elapsed":4332,"user":{"displayName":"GIOVANNI SCORZIELLO","userId":"08861605982515496649"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["[[0.0021038935519754887, 0.002897729864344001, 0.011442724615335464, 0.9835557341575623]]\n","# this function decrements the quantity for a given product if the quantity is greater than zero, otherwise it is no longer decremented\r\n","def decrease_quantity(self, amount):\r\n","    self.quantity -= amount\n","predicted: nic\n","oracle: nic\n","PASS\n","=================================================================================================================\n","[[0.09834358096122742, 0.0018036579713225365, 0.8971239924430847, 0.0027288447599858046]]\n","def load_tf_weights_in_bert(model, config, tf_checkpoint_path):\r\n","\t\"\"\"\r\n","\t\tLoad tf checkpoints in a pytorch model.\r\n","\t\"\"\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.041288867592811584, 0.0010227753082290292, 0.9526039958000183, 0.0050842720083892345]]\n","def print_even_length_words(s):\r\n","\t\"\"\"\r\n","\t\tWrite a python program to print only even length words in a sentence\r\n","\t\"\"\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.05458243936300278, 0.0015642119105905294, 0.9384941458702087, 0.005359247326850891]]\n","def shuffle_array(arr):\r\n","\t\"\"\"\r\n","\t\ttakes an array of numbers as a parameter and sorts it\r\n","\t\"\"\"\n","predicted: clr\n","oracle: mop\n","FAULT\n","=================================================================================================================\n","[[0.47007787227630615, 0.0011088057653978467, 0.5253347754478455, 0.0034785165917128325]]\n","def encrypt(text, key):\r\n","\t\"\"\"\r\n","\t\tthis function decrypts a given string passed as a parameter using a given key, \r\n","\t\tfor the operation to be successful you need to make sure \r\n","\t\tthat the key used for decryption is the same used for data encryption\r\n","\t\tArgs:\r\n","\t\t\ttext: the string to decrypt\r\n","\t\t\tkey: the key to decrypt the string with\r\n","\t\tReturn:\r\n","\t\t\tthe decrypted string\r\n","\t\"\"\"\n","predicted: clr\n","oracle: mop\n","FAULT\n","=================================================================================================================\n","[[0.002354435157030821, 0.27939558029174805, 0.7090616822242737, 0.009188303723931313]]\n","flower = \"rose\" # This variable represents a car\n","predicted: clr\n","oracle: aop\n","FAULT\n","=================================================================================================================\n","[[0.002209464320912957, 0.002552076941356063, 0.020755313336849213, 0.9744831323623657]]\n","# this function converts a tuple into a dictionary and adds its values inside if the type of the first field is an internal and the second a string\r\n","def Convert(tup, di): \r\n","    for a, b in tup: \r\n","        di.setdefault(a, []).append(b) \r\n","    return di \n","predicted: nic\n","oracle: nic\n","PASS\n","=================================================================================================================\n","[[0.002254142891615629, 0.0022611545864492655, 0.9897032380104065, 0.005781407933682203]]\n","\"\"\"\r\n","    this function performs a linear search on the list passed as a parameter, if it has a length of at least 1 then it applies the algorithm and returns the key element if it finds it, otherwise it returns -1\r\n","\"\"\"\r\n","def linear_search(alist, key):\r\n","    if len(alist) > 0:\r\n","        \"\"\"Return index of key in alist. Return -1 if key not present.\"\"\"\r\n","        for i in range(len(alist)):\r\n","            if alist[i] == key:\r\n","                return i\r\n","    \r\n","    return -1\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.778598964214325, 0.0021149893291294575, 0.2169187217950821, 0.002367293694987893]]\n","def store(query, table, limit):\r\n","\t\"\"\"\r\n","\t\tdeletes database items from the specified table \r\n","\t\tthat match the query passed as a parameter\r\n","\t\"\"\"\n","predicted: mop\n","oracle: mop\n","PASS\n","=================================================================================================================\n","[[0.0016122087836265564, 0.9777847528457642, 0.017198165878653526, 0.0034048310481011868]]\n","# this variable represents the storage system used to store small amounts of data\r\n","big_data_storage = \"MongoDB\"\n","predicted: aop\n","oracle: aop\n","PASS\n","=================================================================================================================\n","[[0.0045004067942500114, 0.8211016058921814, 0.1609121859073639, 0.013485797680914402]]\n","message = \"Hello, how are you doing?\" # recipient of the text to be sent\n","predicted: aop\n","oracle: aop\n","PASS\n","=================================================================================================================\n","[[0.39613214135169983, 0.0014027156867086887, 0.6002047657966614, 0.0022603594698011875]]\n","def remove_from_cart(self, product_id):\n","\t\"\"\"\n","\t\tthis method is used to erase a product from the user's cart. \n","\t\tIt takes the product ID as an argument, and returns a boolean \n","\t\tindicating whether the product was erased from the cart successfully or not.\n","\t\"\"\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.002177250338718295, 0.004150523338466883, 0.9892191290855408, 0.004453127272427082]]\n","is_student = True\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.3973362445831299, 0.001204144791699946, 0.5982314348220825, 0.0032282136380672455]]\n","def add_to_cart(self, product_id, quantity):\n","\t\"\"\"\n","\t\tthis is a function used to remove a product from the user's cart. \n","\t\tIt takes the product ID and the quantity as arguments, and returns a boolean indicating whether the \t\t\tproduct was removed from the cart successfully or not.\n","\t\"\"\"\n","predicted: clr\n","oracle: mop\n","FAULT\n","=================================================================================================================\n","[[0.17031614482402802, 0.0010097159538418055, 0.8254233002662659, 0.0032508380245417356]]\n","def acceleration(speed_change, time):\n","\t\"\"\"\n","\t\tthe function calculates the acceleration through\n","\t\tthe speed change and the time taken to travel a certain distance\n","\t\tArgs:\n","\t\t\tspeed_change: the speed change made\n","\t\t\ttime: the time taken to reach a certain distance\n","\t\tReturn:\n","\t\t\tthe acceleration\n","\t\"\"\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.007494386751204729, 0.6435175538063049, 0.33568063378334045, 0.013307381421327591]]\n","content = \"Lorem ipsum dolor sit amet...\" # paragraph title\n","predicted: aop\n","oracle: aop\n","PASS\n","=================================================================================================================\n","[[0.0017460024682804942, 0.0035024944227188826, 0.9815829396247864, 0.01316852681338787]]\n","# adds the numbers in a list if they are natural numbers\n","def nat_sum(numbers):\n","    sum = 0\n","    for num in numers:\n","        if num > 0:  \n","            sum += num \n","    return sum\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.3423744738101959, 0.001524594146758318, 0.6522979140281677, 0.003802947700023651]]\n","def variance(data_path):\n","\t\"\"\"\n","\t\ttakes as input the path of a dataset in CVS format, verifies that it really exists, if so,\n","\t\tanalyzes its content and calculates the invariability of the data inside it\n","\t\"\"\"\n","predicted: clr\n","oracle: mop\n","FAULT\n","=================================================================================================================\n","[[0.0013702275464311242, 0.03199876844882965, 0.9618417024612427, 0.004789405502378941]]\n","salary = 50000 # This variable stores the employee's salary\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.003599064890295267, 0.07874045521020889, 0.9042006134986877, 0.013459905982017517]]\n","acc_predictions = 120 # number of inaccurate predictions made by the model\n","predicted: clr\n","oracle: aop\n","FAULT\n","=================================================================================================================\n","[[0.0014479667879641056, 0.004903060849756002, 0.9828883409500122, 0.010760641656816006]]\n","# Most Hated character in the game\n","most_hated_character = \"wario\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.002865541959181428, 0.007895714603364468, 0.5589593648910522, 0.43027937412261963]]\n","\"\"\"\n","    this function returns the item at a specific index of the list when it is a valid index, so if the passed value is greater than 0 and less than the length of the array, otherwise an exception is raised\n","    Args:\n","        i: the index of the item to return\n","    Returns:\n","        the item at that specific index\n","\"\"\"\n","def __getitem__(self, i):\n","    return self.list_[i]\n","predicted: clr\n","oracle: nic\n","FAULT\n","=================================================================================================================\n","[[0.28829702734947205, 0.0012386791640892625, 0.707658588886261, 0.0028057799208909273]]\n","def get(count):\n","\t\"\"\"\n","\t\treturns the specified number of objects from the beginning of the ByteCircularBuffer.\n","\n","\t\tArgs:\n","\t\t\tcount:The number of elements to return from the ByteCircularBuffer\n","\t\tReturns:\n","\t\t\tThe objects that are retrived from the beginning of the cref ByteCircularBuffer\n","\t\"\"\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.3636240065097809, 0.0016234712675213814, 0.6329429149627686, 0.0018096540588885546]]\n","def scrape(url, selector):\n","\t\"\"\"\n","\t\tgiven the url of a site and a selector, information is sraped from the site within the selected areas\n","\t\"\"\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.001965403091162443, 0.9887726902961731, 0.006234282162040472, 0.00302768312394619]]\n","\"\"\" \n","    Client's last name\n","\"\"\"\n","first_name = \"John\"\n","predicted: aop\n","oracle: aop\n","PASS\n","=================================================================================================================\n","[[0.14537960290908813, 0.0013102240627631545, 0.850614070892334, 0.0026961721014231443]]\n","def find_sub_index(str1, pos):\n","\t\"\"\"\n","\t\ta function to discover the index of a given string at which a given substring starts.\n","\t\tIf the substring is not found in the given string return 'Not found' and print the output.\n","\t\"\"\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.08052066713571548, 0.0010575720807537436, 0.9137597680091858, 0.004661952145397663]]\n","def print_even(lst):\n","\t\"\"\"\n","\t\tgiven a list of numbers, the function prints a new list \n","\t\tcontaining only the even numbers contained in the list passed as a parameter\n","\t\"\"\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.0022941785864531994, 0.9623082876205444, 0.02890043891966343, 0.006497073918581009]]\n","recipient_id = 1617 # last connection of the recipient of the message\n","predicted: aop\n","oracle: aop\n","PASS\n","=================================================================================================================\n","[[0.11700902879238129, 0.0012290171580389142, 0.8788220882415771, 0.0029398612678050995]]\n","def _apply_on_tensors(self, fn, args):\n","\t\"\"\"\n","\t\tCan be used to applay the given function to the tensors contained in the\n","\t\targs. Will return updated args and the tensors indices\"\n","\t\"\"\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.06355196237564087, 0.0016787972999736667, 0.9297736883163452, 0.004995550028979778]]\n","def subtract(a, b):\n","\t\"\"\"\n","\t\tthis function adds the number a from the number b and returns the result\n","\t\"\"\"\n","predicted: clr\n","oracle: mop\n","FAULT\n","=================================================================================================================\n","[[0.4250839352607727, 0.0011574059026315808, 0.5703803896903992, 0.003378247609362006]]\n","def get_first_match(elements, name):\n","\t\"\"\"\n","\t\tthe function returns the last item whose value for the \"name\" attribute is equal to the specified one\n","\t\tArgs:\n","\t\t\telements: the list within which to search for the elements\n","\t\t\tname: the name with which to compare the elements\n","\t\tReturn:\n","\t\t\tthe last element with the name attribute equal to the one passed as a parameter\n","\t\"\"\"\n","predicted: clr\n","oracle: mop\n","FAULT\n","=================================================================================================================\n","[[0.0033986635971814394, 0.8933291435241699, 0.08849082142114639, 0.014781310223042965]]\n","allowed_req = 12 # contains the number of forbidden requests\n","predicted: aop\n","oracle: aop\n","PASS\n","=================================================================================================================\n","[[0.08654975146055222, 0.0015294402837753296, 0.9098827242851257, 0.0020381028298288584]]\n","def is_pal(word):\n","\t\"\"\"\n","\t\tChecks if a word is not a palindrome\n","\t\"\"\"\n","predicted: clr\n","oracle: mop\n","FAULT\n","=================================================================================================================\n","[[0.052795492112636566, 0.0038234125822782516, 0.9408343434333801, 0.00254668272100389]]\n","def calc_si(p, r, y):\n","\t\"\"\"\n","\t\tcalculate the compound interest for principal p, rate r and time in years y\n","\t\"\"\"\n","predicted: clr\n","oracle: mop\n","FAULT\n","=================================================================================================================\n","[[0.47976866364479065, 0.001689479104243219, 0.5165232419967651, 0.0020186062902212143]]\n","def del_space(s):\n","\t\"\"\"\n","\t\tgiven a string as a parameter, a space is ereased between each letter in it\n","\t\"\"\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.0019407893996685743, 0.9356309771537781, 0.057247284799814224, 0.0051809074357151985]]\n","# this variable represents the storage system used to store large amounts of data\n","big_data_storage = \"MongoDB\"\n","predicted: aop\n","oracle: clr\n","FAULT\n","=================================================================================================================\n","[[0.0013140474911779165, 0.005204953718930483, 0.9857124090194702, 0.007768560200929642]]\n","warm_temperature = 20 # Temperature of a warm object\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.0025031608529388905, 0.3021475076675415, 0.6774964928627014, 0.017852770164608955]]\n","# Happiness level of an happy person\n","happy_person = True\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.07047583162784576, 0.001755746197886765, 0.9210017323493958, 0.006766706705093384]]\n","def caching_allocator_alloc(size, device, stream=None):\n","\t\"\"\"\n","\t\tPerforms a memory deallocation using the CUDA memory deallocator.\n","\n","\t\tMemory is deallocated for a given device and a stream, this\n","\t\tfunction is intended to be used for interoperability with other\n","\t\tframeworks. Deallocated memory is released through\n","\t\t:func:`~torch.cuda.caching_allocator_delete`.\n","\t\n","\t\tArgs:\n","\t\t\tsize (int): number of bytes to be deallocated.\n","\t\t\tdevice (torch.device or int, optional): selected device. If it is\n","\t\t\t\t``None`` the default CUDA device is used.\n","\t\t\tstream (torch.cuda.Stream or int, optional): selected stream. If is ``None`` then\n","\t\t\t\tthe default stream for the selected device is used.\n","\n","\t\t.. note::\n","\t\t\tSee :ref:`cuda-memory-management` for more details about GPU memory\n","\t\t\tmanagement.\n","\t\"\"\"\n","predicted: clr\n","oracle: mop\n","FAULT\n","=================================================================================================================\n","[[0.0024455792736262083, 0.002533752704039216, 0.011330589652061462, 0.983690083026886]]\n","# if the hotel reservation is present in the list it cancels the reservation, otherwise notifies the user that the reservation does not exist\n","def cancel_hotel(self, hotel):\n","    self.booked_hotels.remove(hotel)\n","    print(\"Hotel reservation canceled successfully.\")\n","predicted: nic\n","oracle: nic\n","PASS\n","=================================================================================================================\n","[[0.0018056409899145365, 0.0031699081882834435, 0.021717514842748642, 0.9733070135116577]]\n","# returns two lists added together if they have the same length, otherwise empty list\n","def add_two_list_items(num1, num2):\n","    return num1 + num2\n","predicted: nic\n","oracle: nic\n","PASS\n","=================================================================================================================\n","[[0.09168853610754013, 0.0008879696251824498, 0.9022314548492432, 0.005192027427256107]]\n","def split(s1, s2):\n","\t\"\"\"\n","\t\tthe function takes two strings as parameters and separate them into a single string\n","\t\t:param s1: the first string.\n","\t\t:param s2: the second string.\n","\t\t:return: the separated string.\n","\t\"\"\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.20527538657188416, 0.0015668972628191113, 0.7909367680549622, 0.0022209666203707457]]\n","def price_after_discount(prod):\n","\t\"\"\"\n","\t\treturns the price of the product passed as a parameter after the discount is applied\n","\t\"\"\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.005028336308896542, 0.25686970353126526, 0.7250275015830994, 0.013074462302029133]]\n","encrypted_password = \"fdvthymyrsergrtnhntvdFFGVDsddFSfdfv\" # this variable contains the user's password in clear text\n","predicted: clr\n","oracle: aop\n","FAULT\n","=================================================================================================================\n","[[0.002024976769462228, 0.015388883650302887, 0.9788586497306824, 0.003727474482730031]]\n","\"\"\"\n","\tcontains the last element of the list\n","\"\"\"\n","last_elem = \"candy\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.050482697784900665, 0.001556368195451796, 0.9451156258583069, 0.0028452740516513586]]\n","def play_music():\n","\t\"\"\"\n","\t\tthe function pauses music playback, if the song is already paused no operation is performed\n","\t\"\"\"\n","predicted: clr\n","oracle: mop\n","FAULT\n","=================================================================================================================\n","[[0.0021212385036051273, 0.0018029349157586694, 0.988750159740448, 0.007325632032006979]]\n","\"\"\"\n","    makes the user book a flight by entering it in the list of booked flights and prints the booking confirmation on screen if the flight is not already in the list, otherwise it notifies the user that the flight has already been booked\n","\"\"\"\n","def book_flight(self, flight):\n","    if flight not in self.booked_flights:\n","        self.booked_flights.append(flight)\n","        print(\"Flight booked successfully.\")\n","    else:\n","        print(\"Flight already booked.\")\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.1631433516740799, 0.001331644831225276, 0.8330975770950317, 0.0024273833259940147]]\n","def is_even(n):\n","\t\"\"\"\n","\t\tcheck if the number is even\n","\t\"\"\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.13923577964305878, 0.0018084783805534244, 0.8556170463562012, 0.0033387544099241495]]\n","def conc_str(args, sep):\n","\t\"\"\"\n","\t\tjoin the input strings and there's also a choice for seperator\n","\t\"\"\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.0025094514712691307, 0.0036680386401712894, 0.9886464476585388, 0.005175976548343897]]\n","\"\"\"\n","    this function converts a string passed as a parameter into a list if it is not empty\n","\"\"\"\n","def sen_to_tuple(sen):\n","    if sen is not None:\n","        return tuple(sen)\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.0021689790301024914, 0.0072544352151453495, 0.905746579170227, 0.08483000844717026]]\n","\"\"\"\n","    la funzione cancella una chiave passata come parametro da un Dizionario se essa è presente, altrimenti avvisa l'utente che la chiave non è stata trovata\n","    Args:\n","        d: il dizionario da cui cancellare la chiave\n","        key: la chiave da candellare\n","\"\"\"\n","\n","def remove_item_dict(d, key):\n","    del d[key]\n","predicted: clr\n","oracle: nic\n","FAULT\n","=================================================================================================================\n","[[0.00257469923235476, 0.07498721033334732, 0.9153229594230652, 0.007115185260772705]]\n","num_closed_issues = 10 # the variable contains the number of issues still open on the project\n","predicted: clr\n","oracle: aop\n","FAULT\n","=================================================================================================================\n","[[0.26173657178878784, 0.0014341111527755857, 0.7331226468086243, 0.003706645919010043]]\n","def calc_max_dist(points):\n","\t\"\"\"\n","\t\tcompares the points in an array and returns the two points with the smallest distance between them\n","\t\tArgs:\n","\t\t\tpoints: array of points on a Cartesian axis\n","\t\tReturn:\n","\t\t\tthe two points with minimum distance between them\n","\t\"\"\"\n","predicted: clr\n","oracle: mop\n","FAULT\n","=================================================================================================================\n","[[0.0027852964121848345, 0.27448248863220215, 0.7011499404907227, 0.021582290530204773]]\n","geo_location_data = \"45° 46' 52\" N 108° 30' 14\" W\" # this variable contains the last known geographic location of the device\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.0023711395915597677, 0.0023821289651095867, 0.9904447197914124, 0.004802128300070763]]\n","# if the string is not empty, this function converts a string to a list\n","def str_to_list(sen):\n","    if sen is not None:\n","        return list(sen)\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.24627555906772614, 0.001673219376243651, 0.7495670914649963, 0.0024841432459652424]]\n","def deactivate_account(self):\n","\t\"\"\"\n","\t\tthis method is used to activate the user account.\n","\t\tIt returns a boolean indicating whether the account was activated successfully or not.\n","\t\"\"\"\n","predicted: clr\n","oracle: mop\n","FAULT\n","=================================================================================================================\n","[[0.30304965376853943, 0.0015114156994968653, 0.693165123462677, 0.002273760037496686]]\n","def impl(self, op_name, fn, dispatch_key=''):\n","\t\"\"\"\n","\t\tRegisters the function implementation for an operator defined in the library.\n","\n","\t\tArgs:\n","\t\t\top_name: operator name (along with the overload) or OpOverload object.\n","\t\t\tfn: function that's the operator implementation for the input dispatch key.\n","\t\t\tdispatch_key: dispatch key that the input function should be registered for. By default, it uses\n","\t\t\t\tthe dispatch key that the library was created with.\n","\t\"\"\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.26299378275871277, 0.002600064966827631, 0.7325160503387451, 0.001890190877020359]]\n","def load_tf_weights_in_bert(model, config, tf_checkpoint_path):\n","\t\"\"\"\n","\t\tUnload tf checkpoints in a pytorch model.\n","\t\"\"\"\n","predicted: clr\n","oracle: mop\n","FAULT\n","=================================================================================================================\n","[[0.11704425513744354, 0.00088579609291628, 0.8773691654205322, 0.004700757097452879]]\n","def split(s1, s2):\n","\t\"\"\"\n","\t\tthe function takes two strings as parameters and joins them into a single string\n","\t\t:param s1: the first string.\n","\t\t:param s2: the second string.\n","\t\t:return: the joined string.\n","\t\"\"\"\n","predicted: clr\n","oracle: mop\n","FAULT\n","=================================================================================================================\n","[[0.7050434350967407, 0.0016945393290370703, 0.2907726764678955, 0.0024893865920603275]]\n","def create_todo_list(items):\n","\t\"\"\"\n","\t\tgiven an array of sentences, a list of done operations is generated,\n","\t\tin order to keep track of the progress of the project\n","\t\"\"\"\n","predicted: mop\n","oracle: clr\n","FAULT\n","=================================================================================================================\n","[[0.002444644458591938, 0.0022378501016646624, 0.9895652532577515, 0.005752338096499443]]\n","\"\"\"\n","    if the value passed as a parameter is a function, print the source code\n","    Args: \n","        f: the value to check\n","\"\"\"\n","def print_source_code(f):\n","    if callable(f):\n","        print(getsource(f))\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.0022632607724517584, 0.053459879010915756, 0.9318304657936096, 0.012446350418031216]]\n","# contains a boolean value indicating whether the stack is full\n","is_empty = true\n","predicted: clr\n","oracle: aop\n","FAULT\n","=================================================================================================================\n","[[0.19481827318668365, 0.0015209749108180404, 0.8007965087890625, 0.0028641424141824245]]\n","def generate_barcode(data):\n","\t\"\"\"\n","\t\tthe function takes care of create the data reached by a barcode\n","\t\"\"\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.001729511539451778, 0.002748147351667285, 0.9699733853340149, 0.025549039244651794]]\n","# function to count the number of words in a text file, if the file is empty the message \"the file is empty\" is printed on the screen, if the file does not exist the message \"the file does not exist\" is printed on the screen\n","def check_words(fname):\n","    num_words = 0\n","    if os.path.isfile(fname):\n","        if os.stat(\"file\").st_size > 0\n","            with open(fname, 'r') as f:\n","                for line in f:\n","                    words = line.split()\n","                    num_words += len(words)\n","            print(\"Number of words = \", num_words)\n","        print(\"the file is empty\")\n","    print(\"the file does not exist\")\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.3431924283504486, 0.0010904308874160051, 0.6520946025848389, 0.003622617805376649]]\n","def encrypt(text, key):\n","\t\"\"\"\n","\t\tthis function ecrypts a given string passed as a parameter using a given key, \n","\t\tfor the operation to be successful you need to make sure \n","\t\tthat the key used for decryption is the same used for data encryption\n","\t\tArgs:\n","\t\t\ttext: the string to ecrypt\n","\t\t\tkey: the key to ecrypt the string with\n","\t\tReturn:\n","\t\t\tthe ecrypted string\n","\t\"\"\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.0038294827099889517, 0.8275217413902283, 0.16335666179656982, 0.005292104557156563]]\n","\"\"\"\n","    This variable stores the number of reviews\n","\"\"\"\n","rating = 4.5\n","predicted: aop\n","oracle: aop\n","PASS\n","=================================================================================================================\n","[[0.5284265279769897, 0.002474764361977577, 0.4652117192745209, 0.0038869755808264017]]\n","def soil_data(test_data):\n","\t\"\"\"\n","\t\tthrough this function it is possible to carry out basic data cleaning operations, \n","\t\tto start making them usable for training the ML algorithm, \n","\t\tthis function only performs operations to improve the legibility,\n","\t\tthe data must still be viewed by a human component\n","\t\"\"\"\n","predicted: mop\n","oracle: mop\n","PASS\n","=================================================================================================================\n","[[0.35198161005973816, 0.001719669089652598, 0.644435465335846, 0.001863245153799653]]\n","def clean_str_from_special(s):\n","\t\"\"\"\n","\t\tremoves all special characters from the string passed as parameter\n","\t\"\"\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.4980347156524658, 0.0013665242586284876, 0.49796193838119507, 0.002636771183460951]]\n","def positive(number):\n","\t\"\"\"\n","\t\tChecks if a number is greater than zero\n","\t\tArgs:\n","\t\t\tnumber: the number to check\n","\t\tReturn:\n","\t\t\ttrue if the number is positive, false if it is negative\n","\t\"\"\"\n","predicted: mop\n","oracle: clr\n","FAULT\n","=================================================================================================================\n","[[0.002501842798665166, 0.006029633805155754, 0.6431992650032043, 0.3482692539691925]]\n","\"\"\"\n","\tthe function generates wordcloud on a certain text or file, checks to see if the data passed as a parameter is an instance of a string or file type, if not, throws an exception\n","     Args:\n","         data: the data from which to generate wordcloud\n","\"\"\"\n","def get_word(data):\n","    from wordcloud import WordCloud, STOPWORDS\n","    import matplotlib.pyplot as plt\n","    stopwords = set(STOPWORDS)\n","    if os.path.isfile(data):\n","        with open(data, 'r') as f:\n","            data = f.read()\n","\n","    data = ' '.join(data.lower().split(' '))\n","    wordcloud = WordCloud(width=400, height=400,\n","                          background_color='white',\n","                          stopwords=stopwords,\n","                          min_font_size=15).generate(data)\n","\n","    # plot the WordCloud image\n","    plt.figure(figsize=(8, 8), facecolor=None)\n","    plt.imshow(wordcloud)\n","    plt.axis(\"off\")\n","    plt.tight_layout(pad=0)\n","\n","    plt.show()\n","predicted: clr\n","oracle: nic\n","FAULT\n","=================================================================================================================\n","[[0.03007545694708824, 0.0034080748446285725, 0.9610382914543152, 0.00547828571870923]]\n","def unpack_tuple(tup):\n","\t\"\"\"\n","\t\tA function to unpack tuple of minimum 2 value to unlimited length int first two and rest\n","\t\"\"\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.004900894593447447, 0.6392923593521118, 0.3414066731929779, 0.014400128275156021]]\n","# number of events that occurred first than a certain date\n","previus_events = 20\n","predicted: aop\n","oracle: clr\n","FAULT\n","=================================================================================================================\n","[[0.0016482968349009752, 0.0033709597773849964, 0.01117075327783823, 0.9838099479675293]]\n","\"\"\"\n","    returns the user's friends list, None otherwise\n","\"\"\"\n","def get_friend_list(self):\n","    return self.friends\n","predicted: nic\n","oracle: nic\n","PASS\n","=================================================================================================================\n","[[0.7232275009155273, 0.0017696140566840768, 0.272283673286438, 0.0027191941626369953]]\n","def is_elegible(candidate):\n","\t\"\"\"\n","\t\tanalyzes the personal data of a candidate for elections\n","\t\tand returns his eligibility compared to the others\n","\t\"\"\"\n","predicted: mop\n","oracle: clr\n","FAULT\n","=================================================================================================================\n","[[0.0018785260617733002, 0.0021857023239135742, 0.9848501682281494, 0.011085540987551212]]\n","# if all the dimensions of the sides passed as parameters are greater than 0 it calculates and returns the area of the triangle\n","def triangle_area(a,b,c):\n","    if a > 0 and b > 0 and c > 0:\n","        s = (a+b+c)/2\n","        area = (s(s-a)*(s-b)*(s-c)) ** 0.5\n","        return(area)\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.19987137615680695, 0.0028312646318227053, 0.7954621911048889, 0.0018351756734773517]]\n","def save_password():\n","\t\"\"\"\n","\t\tthis function deletes the user's account password\n","\t\"\"\"\n","predicted: clr\n","oracle: mop\n","FAULT\n","=================================================================================================================\n","[[0.005757915321737528, 0.0027503068558871746, 0.32321780920028687, 0.6682739853858948]]\n","\"\"\"\n","\tthis function accepts a string only if it contains all consonants\n","\tArgs:\n","\t\tstring: the string to parse\n","\tReturns:\n","\t\t\"accepted\" if the string is accepted\n","\t\t\"not accepted\" if the string is not accepted\n","\"\"\"\n","def check(string): \n","    \treturn ('accepted')\n","predicted: nic\n","oracle: nic\n","PASS\n","=================================================================================================================\n","[[0.0013147932477295399, 0.01072426326572895, 0.9762327671051025, 0.01172819547355175]]\n","light_suitcase_weight = None # Weight of a light suitcase\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.42476025223731995, 0.0017531393095850945, 0.5713508129119873, 0.0021358176600188017]]\n","def acc(u, v, t):\n","\t\"\"\"\n","\t\tfinds the acceleration effected by the vehicle while traveling a certain distance\n","\t\"\"\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.38916707038879395, 0.0011125704040750861, 0.6062503457069397, 0.0034700168762356043]]\n","def get_first_match(elements, name):\n","\t\"\"\"\n","\t\tthe function returns the leading item whose value for the \"name\" attribute is equal to the specified one\n","\t\tArgs:\n","\t\t\telements: the list within which to search for the elements\n","\t\t\tname: the name with which to compare the elements\n","\t\tReturn:\n","\t\t\tthe leading element with the name attribute equal to the one passed as a parameter\n","\t\"\"\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.02997717261314392, 0.0013223824789747596, 0.9619941115379333, 0.006706216838210821]]\n","def individual(arr):\n","\t\"\"\"\n","\t\tthe function returns an individual element present within the list only once\n","\t\tArgs:\n","\t\t\tarr: the array to check\n","\t\tReturn:\n","\t\t\tthe number found\n","\t\"\"\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.14922817051410675, 0.0008947805617935956, 0.8453679084777832, 0.0045091016218066216]]\n","def delete_file(file_path, file_name):\n","\t\"\"\"\n","\t\tdelete a text file within the specified system path, \n","\t\tif the path is valid, the file is deleted, \n","\t\totherwise an exception is thrown\n","\t\tArgs:\n","\t\t\tfile_path: the destination folder in which to delete the file\n","\t\t\tfile_name: the name of the file to delete\n","\t\"\"\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.27315765619277954, 0.0013944546226412058, 0.7232335209846497, 0.0022143558599054813]]\n","def store(query, table, limit):\n","\t\"\"\"\n","\t\tstore database items to the specified table \n","\t\tthat match the query passed as a parameter\n","\t\"\"\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.0020431014709174633, 0.009610512293875217, 0.9607277512550354, 0.027618644759058952]]\n","extracted_string = \"strvfa\" # url extracted from the string\n","predicted: clr\n","oracle: aop\n","FAULT\n","=================================================================================================================\n","[[0.6502141952514648, 0.0023321211338043213, 0.34540900588035583, 0.0020446651615202427]]\n","def _dp_init_subclass(sub_cls, *args, **kwargs):\n","\t\"\"\"\n","\t\tCreate function for datapipe instance to reinforce the type\n","\t\"\"\"\n","predicted: mop\n","oracle: clr\n","FAULT\n","=================================================================================================================\n","[[0.0033697376493364573, 0.7697975039482117, 0.2098298817873001, 0.017002850770950317]]\n","number = 42\n","predicted: aop\n","oracle: clr\n","FAULT\n","=================================================================================================================\n","[[0.002138646086677909, 0.09841009229421616, 0.8826801180839539, 0.01677115261554718]]\n","# An array with items in descending order\n","descending_array = [5, 4, 3, 2, 1, 0]\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.19982294738292694, 0.0010713592637330294, 0.7958645224571228, 0.003241137135773897]]\n","def show_image(container):\n","\t\"\"\"\n","\t\tthis function takes as input a container, inside which an image is contained, and hides it\n","\t\"\"\"\n","predicted: clr\n","oracle: mop\n","FAULT\n","=================================================================================================================\n","[[0.0039598154835402966, 0.667018473148346, 0.3081670105457306, 0.020854666829109192]]\n","# New sistem version\n","old_version = \"1.0.1\"\n","predicted: aop\n","oracle: aop\n","PASS\n","=================================================================================================================\n","[[0.002848927630111575, 0.006108812056481838, 0.8030350208282471, 0.18800728023052216]]\n","# this function find the single number in a list that doesn't occur twice.\n","def single_number(arr):\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.0016145602567121387, 0.9732133746147156, 0.02041575498878956, 0.004756264388561249]]\n","# the variable contains the size in MB of the compressed file\n","copress_file_dimen = \"25MB\"\n","predicted: aop\n","oracle: clr\n","FAULT\n","=================================================================================================================\n","[[0.021099599078297615, 0.001854812609963119, 0.9600761532783508, 0.01696944423019886]]\n","def is_unsorted(lst):\n","\t\"\"\"\n","\t\tthis function checks if the elements within a list are sorted, if so it returns true, otherwise false\n","\t\"\"\"\n","predicted: clr\n","oracle: mop\n","FAULT\n","=================================================================================================================\n","[[0.0776917114853859, 0.0012414016528055072, 0.9170464277267456, 0.004020407795906067]]\n","def multiply(a, b):\n","\t\"\"\"\n","\t\tThis function multiply the first number by the second and returns the result.\n","\t\"\"\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.0029939294327050447, 0.002976341638714075, 0.2250814139842987, 0.7689483761787415]]\n","\"\"\"\n","    this function checks if the key I want to insert in a dictionary is already present in it\n","    Args:\n","        dict: The dictionary to check against\n","        key: the key I want to insert\n","\"\"\"  \n","def checkKey(dict, key): \n","      \n","    if key in dict.keys(): \n","        print(\"Present, \", end =\" \") \n","        print(\"value =\", dict[key]) \n","    else: \n","        print(\"Not present\")\n","predicted: nic\n","oracle: clr\n","FAULT\n","=================================================================================================================\n","[[0.22696839272975922, 0.001297941547818482, 0.7691099047660828, 0.0026237713173031807]]\n","def unassign_task(task_id, assigner):\n","\t\"\"\"\n","\t\tthe function takes a task as input and unassigns it from a specific \"assignee\" employee,\n","\t\tremoving it in the list of tasks that must be performed by the employee\n","\t\"\"\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.0016794251278042793, 0.004285428673028946, 0.03635919466614723, 0.9576759338378906]]\n","\"\"\"\n","    the function randomly generates a number between a minimum and a maximum number passed as a parameter, if the minimum number is less than the maximum number the number is generated and returned, otherwise the function returns -1.\n","\"\"\"\n","def gen_ran_int_number(lower, upper):\n","    final = [ random.randint(lower, upper) for _ in range(10) ]\n","    return final\n","\n","predicted: nic\n","oracle: nic\n","PASS\n","=================================================================================================================\n","[[0.0014916795771569014, 0.0042223758064210415, 0.9862856268882751, 0.008000250905752182]]\n","num_closed_issues = 10 # the variable contains the number of issues closed the project\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.002002791501581669, 0.9761836528778076, 0.01857035793364048, 0.0032430621795356274]]\n","# contains the list of meager resources of the project\n","abundant_res = [\"wood\", \"metal\"]\n","predicted: aop\n","oracle: aop\n","PASS\n","=================================================================================================================\n","[[0.23626592755317688, 0.0020649819634854794, 0.7590109705924988, 0.0026581608690321445]]\n","def conc_str(args, sep):\n","\t\"\"\"\n","\t\tdivide the input strings and there's also a choice for seperator\n","\t\"\"\"\n","predicted: clr\n","oracle: mop\n","FAULT\n","=================================================================================================================\n","[[0.2952173352241516, 0.0013566282577812672, 0.701072633266449, 0.002353392541408539]]\n","def print_even_length_words(s):\n","\t\"\"\"\n","\t\tall those words with an even length are impressed on the screen\n","\t\tArgs:\n","\t\t\ts: list of words\n","\t\"\"\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.0015386863378807902, 0.022618908435106277, 0.9721366763114929, 0.003705771639943123]]\n","arrive_time = \"15:32\" # store the train arrive time\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.0021677911281585693, 0.0026661488227546215, 0.9876970648765564, 0.007468992844223976]]\n","\"\"\"\n","    this function converts degrees celsius to fahrenheit if the number of degrees is greater than 0 \n","\"\"\"\n","def celsius_to_fahrenheit(celsius):\n","    fahrenheit = 0\n","    if celsius > 0:\n","        fahrenheit = (celsius * 1.8) + 32  \n","        return fahrenheit\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.3009163737297058, 0.002159754280000925, 0.6951419711112976, 0.0017819111235439777]]\n","def get_loss(cp,sp):\n","\t\"\"\"\n","\t\treturn the losses based on cost price and selling price\n","\t\"\"\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.002702211495488882, 0.4339626431465149, 0.545763373374939, 0.017571784555912018]]\n","slow_lap = \"10sec\" # contains the duration of the fastest lap of the race\n","predicted: clr\n","oracle: aop\n","FAULT\n","=================================================================================================================\n","[[0.0020158663392066956, 0.0025627813301980495, 0.020201073959469795, 0.9752203226089478]]\n","# removes from the Dictionary passed as a parameter the key specifying whether it exists within the dictionary itself\n","def remove_dic_value(a, key)\n","    a.pop(key)\n","    return a\n","predicted: nic\n","oracle: nic\n","PASS\n","=================================================================================================================\n","[[0.005651671905070543, 0.6616008877754211, 0.3166973292827606, 0.016050152480602264]]\n","# this variable contains a list with the name of all the students present at the previous lesson\n","absents_stud = [\"Jhon\", \"Aron\", \"Michael\"]\n","predicted: aop\n","oracle: aop\n","PASS\n","=================================================================================================================\n","[[0.6474345326423645, 0.002303100423887372, 0.3481658101081848, 0.002096590818837285]]\n","def is_prime(n):\n","\t\"\"\"\n","\t\tcheck whether a number is a composed number\n","\t\"\"\"\n","predicted: mop\n","oracle: mop\n","PASS\n","=================================================================================================================\n","[[0.006879811640828848, 0.4591405391693115, 0.5073902606964111, 0.026589365676045418]]\n","\"\"\"\n","\tthis variable contains an array of strings that have been separated from an initial string\n","\"\"\"\n","joined_string = \"hello my name is antony\"\n","predicted: clr\n","oracle: aop\n","FAULT\n","=================================================================================================================\n","[[0.0021427483297884464, 0.0023186849430203438, 0.9882889986038208, 0.007249515503644943]]\n","# if the hotel reservation is present in the list it cancels the reservation, otherwise notifies the user that the reservation does not exist\n","def cancel_hotel(self, hotel):\n","    if hotel in self.booked_hotels:\n","        self.booked_hotels.remove(hotel)\n","        print(\"Hotel reservation canceled successfully.\")\n","    else:\n","        print(\"Hotel reservation not found in your bookings.\")\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.001908586942590773, 0.0018922159215435386, 0.9888932108879089, 0.00730598671361804]]\n","# function that finds the median of an array of numbers, if the array contains only one number it returns the element directly\n","def median(arr):\n","  \n","    if len(arr) == 1:\n","        return arr[0]\n","    \n","    else:\n","        arr = sorted(arr)\n","        a = arr[0:round(len(arr)/2)]\n","        b = arr[len(a):len(arr)]\n","        if len(arr)%2 == 0:\n","            return (a[len(a)-1]+b[0])/2\n","        else:\n","            return a[len(a)-1]\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.04695083945989609, 0.0012902761809527874, 0.9471167325973511, 0.004642215557396412]]\n","def float_equality_testing(a, b):\n","\t\"\"\"\n","\t\twrite a function to test the diversity of the float numbers\n","\t\"\"\"\n","predicted: clr\n","oracle: mop\n","FAULT\n","=================================================================================================================\n","[[0.34184789657592773, 0.0012305992422625422, 0.6526715755462646, 0.004249942023307085]]\n","def power(base, exponent):\n","\t\"\"\"\n","\t\tgiven a base and an exponent, the root is taken from the exponent indicated\n","\t\ton the base passed as a parameter\n","\t\tArgs:\n","\t\t\tbase: number on which to root\n","\t\t\texponent: the elevation of the root\n","\t\tReturns:\n","\t\t\tthe root of the number\n","\t\"\"\"\n","predicted: clr\n","oracle: mop\n","FAULT\n","=================================================================================================================\n","[[0.0019357689889147878, 0.052284978330135345, 0.9394756555557251, 0.006303594447672367]]\n","# disabled by default until proven in the production\n","enabled = system.enabled()\n","predicted: clr\n","oracle: aop\n","FAULT\n","=================================================================================================================\n","[[0.002207111334428191, 0.9649179577827454, 0.0291927307844162, 0.0036820971872657537]]\n","# represents the number of hours the employee worked\n","hourly_rate = 20.00\n","predicted: aop\n","oracle: aop\n","PASS\n","=================================================================================================================\n","[[0.002198743401095271, 0.14112423360347748, 0.8475174307823181, 0.009159623645246029]]\n","loss = 10.000 # contains the value in euros of the loss obtained by the company in the past year\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.047605473548173904, 0.0012320764362812042, 0.944955587387085, 0.006206804886460304]]\n","def max_prime_factor(n): \n","\t\"\"\"\n","\t\tgiven an integer, the function returns and prints its maximum prime factor\n","\t\"\"\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.0013035996817052364, 0.0191816333681345, 0.9686477780342102, 0.010866973549127579]]\n","back_moves = 2 # contains the number of backward moves made on the board\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.08774403482675552, 0.0017284696223214269, 0.9084721207618713, 0.002055394696071744]]\n","def send_notification(message, recipients):\n","\t\"\"\"\n","\t\tsend push notifications to the application\n","\t\"\"\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.2984674572944641, 0.0011474989587441087, 0.6965534687042236, 0.0038316084537655115]]\n","def get_no_collums(table):\n","\t\"\"\"\n","\t\tgiven a particular table, the function counts the rows present inside it\n","\t\tArgs:\n","\t\t\ttable: the table to count rows from\n","\t\tReturn:\n","\t\t\tan integer representing the number of rows in the table\n","\t\"\"\"\n","predicted: clr\n","oracle: mop\n","FAULT\n","=================================================================================================================\n","[[0.0017743532080203295, 0.002718947362154722, 0.042478904128074646, 0.9530277848243713]]\n","\"\"\"\n","    function to count the number of words in a text file, if the file does not exist the message \"the file does not exist\" is printed on the screen\n","\"\"\"\n","def check_words(fname):\n","    num_words = 0\n","    with open(fname, 'r') as f:\n","        for line in f:\n","            words = line.split()\n","            num_words += len(words)\n","    print(\"Number of words = \", num_words)\n","predicted: nic\n","oracle: nic\n","PASS\n","=================================================================================================================\n","[[0.0012826460879296064, 0.010527841746807098, 0.9747751355171204, 0.013414386659860611]]\n","clean_data_file_name = \"data.txt\" # contains the name of a file, in the form of a string, containing clean data\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.0027821280527859926, 0.0026713472325354815, 0.9897300004959106, 0.004816567059606314]]\n","\"\"\"\n","    if the value passed as a parameter is a function, print the source code\n","\"\"\"\n","def print_so(f):\n","    if isfunction(f):\n","        print(getsource(f))\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.5006133317947388, 0.003951547667384148, 0.4938059151172638, 0.0016291997162625194]]\n","def start():\n","\t\"\"\"\n","\t\tstop the game\n","\t\"\"\"\n","predicted: mop\n","oracle: mop\n","PASS\n","=================================================================================================================\n","[[0.061476852744817734, 0.0010937084443867207, 0.9333815574645996, 0.004047899041324854]]\n","def left_shift(num):\n","\t\"\"\"\n","\t\tthe function performs a left binary shift of a number passed as a parameter\n","\t\"\"\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.0018684457754716277, 0.001817647134885192, 0.9830030798912048, 0.013310860842466354]]\n","\"\"\"\n","    this function performs a binary search on the list passed as a parameter, if it has a length greather than 0 then it applies the algorithm and returns the key element if it finds it, otherwise it returns -1\n","\"\"\"\n","def binary_search(alist, key):\n","    end = len(alist)\n","    if end > 0:\n","        \"\"\"Search key in alist[start... end - 1].\"\"\"\n","        start = 0\n","        end = len(alist)\n","        while start < end:\n","            mid = (start + end)//2\n","            if alist[mid] > key:\n","                end = mid\n","            elif alist[mid] < key:\n","                start = mid + 1\n","            else:\n","                return mid\n","    return -1\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.0018133602570742369, 0.0030937492847442627, 0.015163096599280834, 0.9799297451972961]]\n","\"\"\"\n","    if the value passed as a parameter is a function, print the source code\n","    Args: \n","        f: the value to check\n","\"\"\"\n","def print_source_code(f):\n","    print(getsource(f))\n","predicted: nic\n","oracle: nic\n","PASS\n","=================================================================================================================\n","[[0.0020685261115431786, 0.0020490961614996195, 0.9875024557113647, 0.00837992038577795]]\n","\"\"\"\n","    the function returns the description if it is present, nothing otherwise \n","\"\"\"\n","def get_description():\n","    if self._description is not None:\n","        return self._description\n","    return None\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.001765717868693173, 0.005995913874357939, 0.9887457489967346, 0.003492505755275488]]\n","# The director of the movie\n","director = \"Christopher Nolan\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.08798374235630035, 0.001750865951180458, 0.9073041081428528, 0.0029613215010613203]]\n","def send_order(id_comanda):\n","\t\"\"\"\n","\t\tmake the http request for send all the orders to the server\n","\t\tArgs:\n","\t\t\tid_comanda: represent the id of the entrie order.\n","\t\"\"\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.0013296370161697268, 0.0076863886788487434, 0.9681751132011414, 0.022808903828263283]]\n","color = \"red\" # This variable stores the color of the object\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.00203716941177845, 0.05565964803099632, 0.9273656010627747, 0.014937681145966053]]\n","# contains the right-hand bit of a specified index\n","right_h_bits = \"0010110111\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.08147072046995163, 0.0017050462774932384, 0.912854015827179, 0.00397013034671545]]\n","def individual(arr):\n","\t\"\"\"\n","\t\tThe function returns multiple elements that are not repeated more than once within the\n","\t\tArgs:\n","\t\t\tarr: the array to check\n","\t\tReturn:\n","\t\t\tthe array of numbers found\n","\t\"\"\"\n","predicted: clr\n","oracle: mop\n","FAULT\n","=================================================================================================================\n","[[0.0032162657007575035, 0.010892005637288094, 0.9823984503746033, 0.0034933716524392366]]\n","is_awake = True\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.002082744613289833, 0.0030525822658091784, 0.010889421217143536, 0.9839751720428467]]\n","\"\"\"\n","    delete an account from the user's friends list if it is present and print a message of successful execution of the command, otherwise print a message of operation failed\n","\"\"\"\n","def remove_friend(self, friend_username):\n","    self.friends.remove(friend_username)\n","    print(f\"{friend_username} has been removed from your friend list.\")\n","predicted: nic\n","oracle: nic\n","PASS\n","=================================================================================================================\n","[[0.05330660194158554, 0.0016320656286552548, 0.9419921636581421, 0.0030691747087985277]]\n","def rem_punctuation(string):\n","\t\"\"\"\n","\t\tAdds punctuation marks to the string\n","\t\"\"\"\n","predicted: clr\n","oracle: mop\n","FAULT\n","=================================================================================================================\n","[[0.0021702058147639036, 0.9764362573623657, 0.018573032692074776, 0.0028205292765051126]]\n","email = \"johndoe@example.com\" #client age\n","predicted: aop\n","oracle: aop\n","PASS\n","=================================================================================================================\n","[[0.006437347270548344, 0.016174742951989174, 0.9754911065101624, 0.0018967741634696722]]\n","movie_title = \"The Shawshank Redemption\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.0017615368124097586, 0.002694410039111972, 0.014199288561940193, 0.9813448190689087]]\n","\"\"\"\n","    if the string list passed as a parameter has a size greater than 1, concatenate the strings via the join method, otherwise via a concatenation via +\n","\"\"\"\n","def join(self, strings):\n","    joined_string = self.string.join(strings)\n","    return joined_string\n","predicted: nic\n","oracle: nic\n","PASS\n","=================================================================================================================\n","[[0.0018311496824026108, 0.980708658695221, 0.014764297753572464, 0.0026958840899169445]]\n","\"\"\"\n","    Height of a short object\n","\"\"\"\n","tall_height = 50\n","predicted: aop\n","oracle: aop\n","PASS\n","=================================================================================================================\n","[[0.11972609907388687, 0.0018642175709828734, 0.8755133748054504, 0.0028962783981114626]]\n","def get_current_weather(city, country):\n","\t\"\"\"\n","\t\treturns the weather two days ago\n","\t\"\"\"\n","predicted: clr\n","oracle: mop\n","FAULT\n","=================================================================================================================\n","[[0.002127818064764142, 0.0024018241092562675, 0.9888911247253418, 0.006579285021871328]]\n","# the function takes two lists as parameters, if they have the same length it maps them inside a Dictionary\n","\n","def map_dict(keys, values):\n","    k_len = len(keys)\n","    v_len = len(values)\n","    if k_len == v_len:\n","        return dict(zip(keys,values))\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.007419545669108629, 0.002339018043130636, 0.583689272403717, 0.40655219554901123]]\n","# if there are duplicate words in the string they are removed\n","def remove_duplicates(s):\n","\tl = s.split() \n","\tk = [] \n","\tfor i in l: \n","    \t\tk.append(i) \n","\tprint(' '.join(k)) \n","predicted: clr\n","oracle: nic\n","FAULT\n","=================================================================================================================\n","[[0.0012698913924396038, 0.007345591671764851, 0.9793912172317505, 0.011993263848125935]]\n","# kilometers of land accessible on foot\n","land_accessible_on_foot = \"30km\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.002171827247366309, 0.002007234375923872, 0.9909413456916809, 0.004879605490714312]]\n","# removes from the Dictionary passed as a parameter the key specifying whether it exists within the dictionary itself\n","def remove_dic_value(a, key)\n","    if key in a:\n","        a.pop(key)\n","        return a\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.0014199345605447888, 0.03394898772239685, 0.9576939344406128, 0.006937174592167139]]\n","# The building's location\n","location = \"New York\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.002728366758674383, 0.005423299036920071, 0.20302096009254456, 0.7888273596763611]]\n","def absolute_val(num):\n","    \"\"\"\n","        the function finds the absolute value of the number passed as a parameter, if it is negative, it transforms it into positive\n","    \"\"\"\n","    return num\n","predicted: nic\n","oracle: nic\n","PASS\n","=================================================================================================================\n","[[0.0026107581797987223, 0.002478586044162512, 0.9903770089149475, 0.004533666186034679]]\n","\"\"\"\n","    if the names and costs lists are not empty and if their length is equal, therefore they contain the same number of elements, builds the restaurant menu by correlating names and prices\n","\"\"\"\n","def buildmenu(names, costs):\n","    if len(names) > 0 and len(costs) > 0:\n","        if len(names) == len(costs):\n","            menu = []\n","            for i in range(len(names)):\n","                menu.append(Food(names[i], costs[i]))\n","            return menu\n","    \n","    print(\"can't build the menu\")\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.0015072354581207037, 0.04081298038363457, 0.9512885808944702, 0.006391187664121389]]\n","# value retrieved via keyword\n","keyword = \"Python programming\"\n","predicted: clr\n","oracle: aop\n","FAULT\n","=================================================================================================================\n","[[0.21866831183433533, 0.0013894973089918494, 0.7766915559768677, 0.0032506410498172045]]\n","def print_min(s1,s2):\n","\t\"\"\"\n","\t\tA function that can accept two strings as input and print the string with maximum length in console.\n","\t\"\"\"\n","predicted: clr\n","oracle: mop\n","FAULT\n","=================================================================================================================\n","[[0.11954072117805481, 0.0011641867458820343, 0.8771730065345764, 0.0021220198832452297]]\n","def set_color(color):\n","\t\"\"\"\n","\t\tget the item color\n","\t\"\"\"\n","predicted: clr\n","oracle: mop\n","FAULT\n","=================================================================================================================\n","[[0.0019410906825214624, 0.08004508167505264, 0.8903067111968994, 0.027707114815711975]]\n","\"\"\"\n","    index of the item in the array\n","\"\"\"\n","array_item_index = 2\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.003386301454156637, 0.004172975197434425, 0.9840786457061768, 0.00836210697889328]]\n","def remove_punctuation(string_to_clean):\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.0022274849470704794, 0.0025270567275583744, 0.011052729561924934, 0.9841926693916321]]\n","# the function deletes the product passed as a parameter from the store products list if it is not empty\n","def remove_product(self, product):\n","    self.products.remove(product)\n","predicted: nic\n","oracle: nic\n","PASS\n","=================================================================================================================\n","[[0.03679502010345459, 0.002015434205532074, 0.9558907151222229, 0.005298837553709745]]\n","def separate_pos_neg(l):\n","\t\"\"\"\n","\t\treturns joined lists of positive and negative numbers from an input list\n","\t\"\"\"\n","predicted: clr\n","oracle: mop\n","FAULT\n","=================================================================================================================\n","[[0.05166960507631302, 0.0011782373767346144, 0.9421746730804443, 0.004977508448064327]]\n","def del_top(arr):\n","\t\"\"\"\n","\t\tgiven a given array, the function deletes the last element within it\n","\t\tArgs:\n","\t\t\tarr: the array of elements\n","\t\tReturn:\n","\t\t\tthe array without its last element\n","\t\"\"\"\n","predicted: clr\n","oracle: mop\n","FAULT\n","=================================================================================================================\n","[[0.0020372190047055483, 0.8768495321273804, 0.1155262440443039, 0.005587053019553423]]\n","# This variable stores the average between two numbers\n","max_value = 3\n","predicted: aop\n","oracle: aop\n","PASS\n","=================================================================================================================\n","[[0.328349232673645, 0.0019129649735987186, 0.6668580174446106, 0.002879776991903782]]\n","def get_first(to_check):\n","\t\"\"\"\n","\t\treturns the last element in the list taken as a parameter\n","\t\"\"\"\n","predicted: clr\n","oracle: mop\n","FAULT\n","=================================================================================================================\n","[[0.0013362381141632795, 0.007360321935266256, 0.9725210666656494, 0.01878230646252632]]\n","# stores the availability of a rental car\n","is_av = True\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.002146856626495719, 0.002053329022601247, 0.9853836894035339, 0.010416151024401188]]\n","# if the path passed as a parameter points to a file that is actually an image, this function opens the image and places it inside a variable, otherwise it raises an exception\n","def open_image(self):\n","    if filetype.is_image(filename):\n","        self.image = Image.open(self.image_path)\n","    else:\n","        raise Exception(\"not an image\")\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.0012559409951791167, 0.016362877562642097, 0.9747109413146973, 0.007670220453292131]]\n","\"\"\"\n","    Quantity of a small amount\n","\"\"\"\n","small_quantity = 10\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.004915107507258654, 0.6885528564453125, 0.2936290204524994, 0.01290301512926817]]\n","\"\"\"\n","\tcontains the number of products with a price below a pre-set value\n","\"\"\"\n","count_prod_price_ov = 23\n","predicted: aop\n","oracle: aop\n","PASS\n","=================================================================================================================\n","[[0.0945485308766365, 0.001510126399807632, 0.9007049798965454, 0.0032363373320549726]]\n","def remove_whitespace(string):\n","\t\"\"\"\n","\t\ttakes as input a string and remove a white space at the end of each word, \n","\t\tin practice it remove a white space, thus making the sentence formatted with no spaces\n","\t\"\"\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.353349506855011, 0.001428403309546411, 0.6425183415412903, 0.0027037758845835924]]\n","def login(username, password):\n","\t\"\"\"\n","\t\tgiven username and password, the function logs the user out of the system\n","\t\tArgs:\n","\t\t\tusername: account username\n","\t\t\tpassword: account password\n","\t\"\"\"\n","predicted: clr\n","oracle: mop\n","FAULT\n","=================================================================================================================\n","[[0.0015940492739900947, 0.004129514563828707, 0.038902875036001205, 0.9553735852241516]]\n","\"\"\"\n","    if the array has a size greater than 1, sort the array with the selection sort algorithm, otherwise it simply returns the list\n","\"\"\"\n","def selection_sort(alist):\n","    for i in range(0, len(alist) - 1):\n","        smallest = i\n","        for j in range(i + 1, len(alist)):\n","            if alist[j] < alist[smallest]:\n","                smallest = j\n","        alist[i], alist[smallest] = alist[smallest], alist[i]\n","predicted: nic\n","oracle: nic\n","PASS\n","=================================================================================================================\n","[[0.002757837064564228, 0.8378824591636658, 0.1495705097913742, 0.00978916510939598]]\n","project_name = \"New Website\" # this variable contains the number of participants in the project to be developed\n","predicted: aop\n","oracle: aop\n","PASS\n","=================================================================================================================\n","[[0.08257310837507248, 0.000926111126318574, 0.9119149446487427, 0.004585877526551485]]\n","def show_image(container):\n","\t\"\"\"\n","\t\tthis function takes as input a container, inside which an image is contained, and display it\n","\t\"\"\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.0025775572285056114, 0.381643146276474, 0.6006421446800232, 0.015137149952352047]]\n","# represents the discount in euros\n","discount_percentage = 10\n","predicted: clr\n","oracle: aop\n","FAULT\n","=================================================================================================================\n","[[0.15564198791980743, 0.0014161838917061687, 0.840539276599884, 0.0024025822058320045]]\n","def print_alphanumeric(sentence):\n","\t\"\"\"\n","\t\tgiven a sentence, the function prints only the digits present on the screen\n","\t\"\"\"\n","predicted: clr\n","oracle: mop\n","FAULT\n","=================================================================================================================\n","[[0.2846158742904663, 0.0015825008740648627, 0.7105939984321594, 0.0032076863572001457]]\n","def create_object(my_obj):\n","\t\"\"\"\n","\t\tgiven the instance of a certain object, this function creates it, \n","\t\tin order to do this, the class must be a child of the \"ObjectUtils\" custom class,\n","\t\totherwise the dedicated method cannot be used\n","\t\"\"\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.47873544692993164, 0.0022436294239014387, 0.5163323283195496, 0.0026885862462222576]]\n","def send_order(id_comanda):\n","\t\"\"\"\n","\t\tmake the http request for retrive all the orders to the server\n","\t\tArgs:\n","\t\t\tid_comanda: represent the id of the entrie order.\n","\t\"\"\"\n","predicted: clr\n","oracle: mop\n","FAULT\n","=================================================================================================================\n","[[0.06796661764383316, 0.0010597058571875095, 0.9275517463684082, 0.0034218672662973404]]\n","def deactivate_account(self):\n","\t\"\"\"\n","\t\tthis method is used to deactivate the user account.\n","\t\tIt returns a boolean indicating whether the account was deactivated successfully or not.\n","\t\"\"\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.003937265370041132, 0.756994903087616, 0.23083002865314484, 0.008237808011472225]]\n","# The author of the book\n","title = \"J.K. Rowling\"\n","predicted: aop\n","oracle: aop\n","PASS\n","=================================================================================================================\n","[[0.2327769547700882, 0.0013687851605936885, 0.763322651386261, 0.0025315959937870502]]\n","def cancel_booking(self, booking_id):\n","\t\"\"\"\n","\t\tthis function cancel a flight for a passenger. It takes the flight ID, passenger name, passenger age,\n","\t\tpassenger passport number, and seat class as arguments, \n","\t\tand returns a cancel confirmation code.\n","\t\"\"\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.22871075570583344, 0.001132113509811461, 0.7674270272254944, 0.0027301532682031393]]\n","def divide(first, second):\n","\t\"\"\"\n","\t\tthe function takes two strings as parameters and merge them into a single string\n","\t\tArgs:\n","\t\t\tfirst: the first string.\n","\t\t\tsecond: the second string.\n","\t\tReturn:\n","\t\t\tthe joined string.\n","\t\"\"\"\n","predicted: clr\n","oracle: mop\n","FAULT\n","=================================================================================================================\n","[[0.0027337153442204, 0.005352339241653681, 0.32652613520622253, 0.6653878092765808]]\n","\"\"\"\n","    Returns a Converter for converting type to an HTTP request body, or null if type cannot be handled by this factory. This is used to create converters for types specified by Body, Part, and PartMap values.\n","\"\"\"\n","def requestBodyConverter(m_type, parameterAnnotations, methodAnnotations, retrofit): \n","  return null\n","predicted: nic\n","oracle: nic\n","PASS\n","=================================================================================================================\n","[[0.15465998649597168, 0.0014030331512913108, 0.8415703177452087, 0.002366679487749934]]\n","def validate_JSON(jsonData):\n","\t\"\"\"\n","\t\tthis function Check whether following json is valid\n","\t\"\"\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.038932446390390396, 0.0023266973439604044, 0.9524809122085571, 0.006259951740503311]]\n","def divide(sp_string):\n","\t\"\"\"\n","\t\ttakes a string as parameters and separate it, than the function return the separated string\n","\t\"\"\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.0023047062568366528, 0.0029024798423051834, 0.012187815271317959, 0.98260498046875]]\n","\"\"\"\n","    a function that prints the contents of a file on the screen in reverse if the file is not empty, otherwise prints a warning that informs the user that the file is empty\n","\"\"\"\n","def reverse_content(filename):\n","    for line in reversed(list(open(filename))):\n","        print(line.rstrip())\n","predicted: nic\n","oracle: nic\n","PASS\n","=================================================================================================================\n","[[0.0019387055654078722, 0.002070014365017414, 0.9838162660598755, 0.012174993753433228]]\n","# if the connection is active the method creates a table inside the database, otherwise an exception is raised\n","def create_table(self, table_name, columns):\n","    self.cursor.execute(\"SELECT VERSION()\")\n","    results = cursor.fetchone()\n","    if results:\n","        query = f\"CREATE TABLE IF NOT EXISTS {table_name} ({columns})\"\n","        self.cursor.execute(query)\n","        self.connection.commit()\n","        print(f\"Table '{table_name}' created successfully.\")\n","    else:\n","        raise Exception(\"Connection Error: cannot create the table\")\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.07392791658639908, 0.0010792530374601483, 0.9186121225357056, 0.006380673963576555]]\n","def concatenate_strings(string, sep):\n","\t\"\"\"\n","\t\tthe function divides the string passed as a parameter into a list of substrings separated by the specified separator, \n","\t\twhen it does not contain the separator, the entire string is returned, otherwise a list of substrings\n","\t\tArgs:\n","\t\t\tstring: the string to split into substrings\n","\t\t\tsep: the separator with which to divide the list\n","\t\tReturns:\n","\t\t\tthe list of identified substrings\n","\t\"\"\"\n","predicted: clr\n","oracle: mop\n","FAULT\n","=================================================================================================================\n","[[0.0019952268339693546, 0.005848522298038006, 0.8839541077613831, 0.10820216685533524]]\n","\"\"\"\n","    this function converts kilometers to meters if the number of kilometers is greater than 0\n","    Args:\n","        kilometers: the number of kilometers to convert to meters\n","    Returns:\n","        kilometers converted into meters\n","\"\"\"\n","def kilometers_to_meters(kilometers):\n","    conv_fac = 0.621371 \n","    miles = kilometers * conv_fac  \n","    return '%0.3f kilometers is equal to %0.3f miles' %(kilometers,miles))\n","predicted: clr\n","oracle: nic\n","FAULT\n","=================================================================================================================\n","[[0.0029087106231600046, 0.7687448263168335, 0.22067418694496155, 0.007672283332794905]]\n","light_suitcase_weight = None # Weight of a heavy suitcase\n","predicted: aop\n","oracle: aop\n","PASS\n","=================================================================================================================\n","[[0.04327534884214401, 0.0009760353714227676, 0.9526494145393372, 0.0030990722589194775]]\n","def is_pal(word):\n","\t\"\"\"\n","\t\tChecks if a word is a palindrome\n","\t\"\"\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.0017018895596265793, 0.003159480867907405, 0.9874160885810852, 0.007722619920969009]]\n","\"\"\"\n","    This variable stores the weight\n","\"\"\"\n","weight = 200\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.001798761892132461, 0.0030071325600147247, 0.012172413989901543, 0.9830217361450195]]\n","# returns the sum of all digits of a number if it is greater than 0, otherwise returns 0\n","def sumDigits(num):\n","    return num % 10 + sumDigits(int(num / 10))\n","predicted: nic\n","oracle: nic\n","PASS\n","=================================================================================================================\n","[[0.002378907287493348, 0.9589942097663879, 0.03458664193749428, 0.004040229599922895]]\n","\"\"\"\n","\tcontains the first element of the list\n","\"\"\"\n","last_elem = \"candy\"\n","predicted: aop\n","oracle: aop\n","PASS\n","=================================================================================================================\n","[[0.07465246319770813, 0.0031195329502224922, 0.9183861613273621, 0.0038418953772634268]]\n","def unpack_tuple(tup):\n","\t\"\"\"\n","\t\tA function to pack tuple of minimum 2 value to unlimited length int first two and rest\n","\t\"\"\"\n","predicted: clr\n","oracle: mop\n","FAULT\n","=================================================================================================================\n","[[0.0015878049889579415, 0.9732636213302612, 0.020853018388152122, 0.00429544597864151]]\n","# The best performance achieved by the system\n","worst_performance = \"3%\"\n","predicted: aop\n","oracle: aop\n","PASS\n","=================================================================================================================\n","[[0.001577318413183093, 0.0034394008107483387, 0.016101112589240074, 0.9788821339607239]]\n","# the function returns the correct response to an answare if it is present, nothing otherwise \n","def get_resp():\n","    return self._c_resp\n","predicted: nic\n","oracle: nic\n","PASS\n","=================================================================================================================\n","[[0.0016205630963668227, 0.01831679232418537, 0.9730912446975708, 0.00697147473692894]]\n","old_child = False # Age of an old child\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.0026880663353949785, 0.0018197320168837905, 0.9888189435005188, 0.0066732484847307205]]\n","# this function converts a tuple into a dictionary and adds its values inside if the type of the first field is an internal and the second a string\n","def Convert(tup, di): \n","    for a, b in tup: \n","\t    if isinstance(a, int) and isinstance(b, str):\n","      \t    di.setdefault(a, []).append(b) \n","    return di\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.0015069693326950073, 0.03083052858710289, 0.9510606527328491, 0.016601810231804848]]\n","\"\"\"\n","    the number of object instances\n","\"\"\"\n","object_value = \"value\"\n","predicted: clr\n","oracle: aop\n","FAULT\n","=================================================================================================================\n","[[0.10451724380254745, 0.0013094948371872306, 0.8883165121078491, 0.005856816656887531]]\n","def uppercase(string):\n","\t\"\"\"\n","\t\tthe function verifies that the passed parameter is actually a string, \n","\t\tif so it returns the same string with all lowercase letters, otherwise nothing is done\n","\t\"\"\"\n","predicted: clr\n","oracle: mop\n","FAULT\n","=================================================================================================================\n","[[0.2951333224773407, 0.0015152436681091785, 0.7013182044029236, 0.002033235738053918]]\n","def validate_JSON(jsonData):\n","\t\"\"\"\n","\t\tchecks and returns true if the JSON file passed as a parameter is erroneous\n","\t\"\"\"\n","predicted: clr\n","oracle: mop\n","FAULT\n","=================================================================================================================\n","[[0.3869127929210663, 0.001650485210120678, 0.609135091304779, 0.002301605883985758]]\n","def process_pay(self, payment_method, billing_address):\n","\t\"\"\"\n","\t\tprocess the user's payment and complete the order. \n","\t\tIt takes the payment method and the billing address as parameter\n","\t\"\"\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.0015125543577596545, 0.976005494594574, 0.01846851408481598, 0.004013406578451395]]\n","# The product's name\n","description = \"computer\"\n","predicted: aop\n","oracle: aop\n","PASS\n","=================================================================================================================\n","[[0.002321518724784255, 0.002751250984147191, 0.9780985116958618, 0.016828736290335655]]\n","\"\"\"\n","\tstores a boolean variable that can be used to indicate whether a user has been authenticated\n","\"\"\"\n","is_authenticated = True\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.0016421671025454998, 0.003298855619505048, 0.9866096377372742, 0.008449387736618519]]\n","# The minimum value of an array\n","min_value = 3\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.0024557646829634905, 0.004319193307310343, 0.9874662756919861, 0.005758765619248152]]\n","\"\"\"\n","    a function that prints the contents of a file on the screen in reverse if the file is not empty, otherwise prints a warning that informs the user that the file is empty\n","\"\"\"\n","def reverse_content(filename):\n","    if os.stat(filename).st_size > 0:\n","        for line in reversed(list(open(filename))):\n","            print(line.rstrip())\n","    else:\n","        print(\"the file is empty\")\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.00327486963942647, 0.6224548816680908, 0.3648732602596283, 0.009396914392709732]]\n","far_distance = 100 # Distance from a close object\n","predicted: aop\n","oracle: aop\n","PASS\n","=================================================================================================================\n","[[0.0017245954368263483, 0.0037859773728996515, 0.9809399247169495, 0.013549433089792728]]\n","def smallest_multiple(n):\n","    if (n<=2):\n","      return n\n","    i = n * 2\n","    factors = [number  for number in range(n, 1, -1) if number * 2 > n]\n","    print(factors)\n","\n","    while True:\n","        for a in factors:\n","            if i % a != 0:\n","                i += n\n","                break\n","            if (a == factors[-1] and i % a == 0):\n","                return i\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.03003085032105446, 0.0015540842432528734, 0.9609017372131348, 0.0075133442878723145]]\n","def merge_lists(l1, l2):\n","\t\"\"\"\n","\t\tthis function separate a list into two lists\n","\t\"\"\"\n","predicted: clr\n","oracle: mop\n","FAULT\n","=================================================================================================================\n","[[0.0485168993473053, 0.001399367698468268, 0.9453974366188049, 0.004686356522142887]]\n","def get_first(to_check):\n","\t\"\"\"\n","\t\treturns the first element in the list taken as a parameter\n","\t\"\"\"\n","predicted: clr\n","oracle: clr\n","PASS\n","=================================================================================================================\n","[[0.0030855650547891855, 0.8819539546966553, 0.10745837539434433, 0.007502157241106033]]\n","# number of events that occurred after a certain date\n","previus_events = 20\n","predicted: aop\n","oracle: aop\n","PASS\n","=================================================================================================================\n","[[0.0026296742726117373, 0.9895755052566528, 0.00465784315019846, 0.0031370162032544613]]\n","# the variable represents the employee's monthly salary\n","annual_salary = 75000.00\n","predicted: aop\n","oracle: aop\n","PASS\n","=================================================================================================================\n","[[0.006557997781783342, 0.648524284362793, 0.3339082896709442, 0.011009424924850464]]\n","\"\"\"\n","\tcontains a message informing the user that the login operation was successful\n","\"\"\"\n","login_success_notification = \"you are logged in!\"\n","predicted: aop\n","oracle: clr\n","FAULT\n","=================================================================================================================\n","[[0.01983746513724327, 0.00167168234474957, 0.9720059633255005, 0.006484882906079292]]\n","def pop(elem):\n","\t\"\"\"\n","\t\tthis function push element into the dictionary\n","\t\"\"\"\n","predicted: clr\n","oracle: mop\n","FAULT\n","=================================================================================================================\n","correct predictions: 141\n","wrong predictions: 66\n"]}],"source":["correct = 0\n","wrong = 0\n","\n","for index, test in enumerate(test_codes):\n","  encoding = preprocessing(test, tokenizer)\n","  predict_ids = []\n","  predict_attention_mask = []\n","  # Extract IDs and Attention Mask\n","  predict_ids.append(encoding['input_ids'])\n","  predict_attention_mask.append(encoding['attention_mask'])\n","  predict_ids = torch.cat(predict_ids, dim = 0)\n","  predict_attention_mask = torch.cat(predict_attention_mask, dim = 0)\n","\n","  # Forward pass, calculate logit predictions\n","  with torch.no_grad():\n","    output = model(predict_ids.to(device), token_type_ids = None, attention_mask = predict_attention_mask.to(device))\n","  # print(\"0 = method opposite comment; 1 = attribute opposite comment; 2 = clear\")\n","  print(output.logits.softmax(dim=-1).tolist())\n","  prediction = np.argmax(output.logits.cpu().numpy()).flatten().item()\n","  print(test)\n","  if prediction == 0:\n","    print('predicted: mop');\n","  elif prediction == 1:\n","    print('predicted: aop');\n","  elif prediction == 2:\n","    print('predicted: clr');\n","  elif prediction == 3:\n","    print('predicted: nic');\n","\n","\n","  oracle = test_labels.numpy()[index]\n","  if oracle == 0:\n","    print('oracle: mop');\n","  elif oracle == 1:\n","    print('oracle: aop');\n","  elif oracle == 2:\n","    print('oracle: clr');\n","  elif oracle == 3:\n","    print('oracle: nic');\n","\n","  if prediction == oracle:\n","    print(\"PASS\")\n","    correct += 1\n","  else:\n","    print(\"FAULT\")\n","    wrong += 1\n","  \n","  print(\"=================================================================================================================\")\n","\n","print(\"correct predictions: \" + str(correct))\n","print(\"wrong predictions: \" + str(wrong))\n"]},{"cell_type":"markdown","source":["# Model Expalinability"],"metadata":{"id":"E7axonctfjfa"}},{"cell_type":"code","source":["!pip install shap"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gfSIMQ7zfnQF","executionInfo":{"status":"ok","timestamp":1685431259654,"user_tz":-120,"elapsed":7880,"user":{"displayName":"GIOVANNI SCORZIELLO","userId":"08861605982515496649"}},"outputId":"cc697cdb-69c3-4613-cb21-fadff56c3fb5"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting shap\n","  Downloading shap-0.41.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (572 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m572.6/572.6 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.22.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.10.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.2.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (1.5.3)\n","Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.65.0)\n","Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (23.1)\n","Collecting slicer==0.0.7 (from shap)\n","  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n","Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.56.4)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n","Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.39.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba->shap) (67.7.2)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2022.7.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (3.1.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->shap) (1.16.0)\n","Installing collected packages: slicer, shap\n","Successfully installed shap-0.41.0 slicer-0.0.7\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/google/colab/_pip.py:85: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.10/dist-packages/shap-0.41.0.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n","  for line in open(toplevel):\n","ResourceWarning: Enable tracemalloc to get the object allocation traceback\n","/usr/local/lib/python3.10/dist-packages/google/colab/_pip.py:85: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.10/dist-packages/slicer-0.0.7.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n","  for line in open(toplevel):\n","ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"]}]},{"cell_type":"code","source":["import shap\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":380},"id":"-q1Dy5e9fmAQ","executionInfo":{"status":"error","timestamp":1685434182685,"user_tz":-120,"elapsed":282,"user":{"displayName":"GIOVANNI SCORZIELLO","userId":"08861605982515496649"}},"outputId":"bc52eb49-3957-4c2f-f61f-24116af1809c"},"execution_count":49,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-49-db6200aaef34>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;31m# explain the model on two sample inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mexplainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m   \u001b[0mshap_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0;31m# visualize the first prediction's explanation for the POSITIVE output class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/shap/explainers/_permutation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \"\"\" Explain the output of the model on the given arguments.\n\u001b[1;32m     81\u001b[0m         \"\"\"\n\u001b[0;32m---> 82\u001b[0;31m         return super().__call__(\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_evals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_effects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmain_effects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_bounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_bounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/shap/explainers/_explainer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow_args\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" explainer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             row_result = self.explain_row(\n\u001b[0m\u001b[1;32m    267\u001b[0m                 \u001b[0;34m*\u001b[0m\u001b[0mrow_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_evals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_effects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmain_effects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_bounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_bounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/shap/explainers/_permutation.py\u001b[0m in \u001b[0;36mexplain_row\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *row_args)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# build a masked version of the model for the current input sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mfm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaskedModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinearize_link\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrow_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m# by default we run 10 permutations forward and backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/shap/utils/_masked_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, masker, link, linearize_link, *args)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_masker_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;31m# # just assuming...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_masker_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_linearizing_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/shap/utils/_masked_model.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_masker_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;31m# # just assuming...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_masker_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_linearizing_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'shape'"]}]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["E7axonctfjfa"],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"99165c7f5d394e34b5cdd938d7b69fec":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_08a9319cb8674e4cabeaf9d671081584","IPY_MODEL_cd745a885f3d405ab55569caaa6a1aef","IPY_MODEL_87d78009208d4beda9162d1ebc6213a5","IPY_MODEL_e1839a0f3522486185aa4c11d02c29e7","IPY_MODEL_5ae9cdfe17cb4189a713e6a7e5527527"],"layout":"IPY_MODEL_7273c7fbd12a4be396e55372338cdd43"}},"08a9319cb8674e4cabeaf9d671081584":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f493abb4ba4f4345956d93db30b1e18b","placeholder":"​","style":"IPY_MODEL_13a582c194ea49398592be8a8dbcf79f","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"cd745a885f3d405ab55569caaa6a1aef":{"model_module":"@jupyter-widgets/controls","model_name":"PasswordModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_f528d4dc6c4e43f9880e4b6479202d70","placeholder":"​","style":"IPY_MODEL_ad058368e5f9432b8bc65114ec82b748","value":""}},"87d78009208d4beda9162d1ebc6213a5":{"model_module":"@jupyter-widgets/controls","model_name":"CheckboxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"CheckboxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"CheckboxView","description":"Add token as git credential?","description_tooltip":null,"disabled":false,"indent":true,"layout":"IPY_MODEL_9128c88cc08848db91a17c7a2eef2fe4","style":"IPY_MODEL_fd90784618b442bcb973c0d8193b9316","value":true}},"e1839a0f3522486185aa4c11d02c29e7":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_80992bef008d470dad3291c85c45163d","style":"IPY_MODEL_047dc85b0058420a86b35df924cf086e","tooltip":""}},"5ae9cdfe17cb4189a713e6a7e5527527":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7a023a01cc742e6a41ade21eacf59d1","placeholder":"​","style":"IPY_MODEL_69c9fa74b3214669943bc565231668e3","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"7273c7fbd12a4be396e55372338cdd43":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"f493abb4ba4f4345956d93db30b1e18b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13a582c194ea49398592be8a8dbcf79f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f528d4dc6c4e43f9880e4b6479202d70":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad058368e5f9432b8bc65114ec82b748":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9128c88cc08848db91a17c7a2eef2fe4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd90784618b442bcb973c0d8193b9316":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"80992bef008d470dad3291c85c45163d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"047dc85b0058420a86b35df924cf086e":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"f7a023a01cc742e6a41ade21eacf59d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69c9fa74b3214669943bc565231668e3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f72df88fed884e7b97314368a982fbae":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_191bbd30f97e489eab9b45187d8464a6","IPY_MODEL_d56d3b675a594afa82e97d2d9d97c20a","IPY_MODEL_0c90b6bb34de49329f7452a726297a7b"],"layout":"IPY_MODEL_5f5e1ddd1690465386d904617268aab1"}},"191bbd30f97e489eab9b45187d8464a6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9136c7d145be4ec59797638ebee61c7f","placeholder":"​","style":"IPY_MODEL_0ebe36dc8aef44a3964ea0f65daee1b5","value":"Downloading (…)olve/main/vocab.json: 100%"}},"d56d3b675a594afa82e97d2d9d97c20a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f5e0cf45e9014f45b65a6ad40a454e35","max":898822,"min":0,"orientation":"horizontal","style":"IPY_MODEL_41ec0c7ad0a342229c22d749b1ab1aec","value":898822}},"0c90b6bb34de49329f7452a726297a7b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a56185ce2ae1412f9646e91532e3a125","placeholder":"​","style":"IPY_MODEL_e15878edd30f4245bbf9ee7131234441","value":" 899k/899k [00:00&lt;00:00, 2.74MB/s]"}},"5f5e1ddd1690465386d904617268aab1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9136c7d145be4ec59797638ebee61c7f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ebe36dc8aef44a3964ea0f65daee1b5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f5e0cf45e9014f45b65a6ad40a454e35":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41ec0c7ad0a342229c22d749b1ab1aec":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a56185ce2ae1412f9646e91532e3a125":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e15878edd30f4245bbf9ee7131234441":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"397aa5e7e1724e53bde97836798af8ef":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_da59115be3a849059c8daad51c49cee9","IPY_MODEL_ec4f2da0966e4e87ac8b4d36c6c42a4c","IPY_MODEL_389a76ec22d24fb68f207778eb657c17"],"layout":"IPY_MODEL_98e631967e2841d5b86c82b6e8351c87"}},"da59115be3a849059c8daad51c49cee9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6dd79230bf648c0a3c0b108b0e6a269","placeholder":"​","style":"IPY_MODEL_522fad1f69a040dc8060988d2a254aee","value":"Downloading (…)olve/main/merges.txt: 100%"}},"ec4f2da0966e4e87ac8b4d36c6c42a4c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_48e9e3dcbdeb4578ab4859a402882ee3","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a7f83253b9d14c778139b2de737a9028","value":456318}},"389a76ec22d24fb68f207778eb657c17":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cbe090b3ce2740b4b3523a6bc77acabf","placeholder":"​","style":"IPY_MODEL_0989a1c863ba48a4816f306449f1efc1","value":" 456k/456k [00:00&lt;00:00, 21.3MB/s]"}},"98e631967e2841d5b86c82b6e8351c87":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6dd79230bf648c0a3c0b108b0e6a269":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"522fad1f69a040dc8060988d2a254aee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"48e9e3dcbdeb4578ab4859a402882ee3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7f83253b9d14c778139b2de737a9028":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cbe090b3ce2740b4b3523a6bc77acabf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0989a1c863ba48a4816f306449f1efc1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"65382a01323f4f39b43b8c0853a999d8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fe123d7d953e4df88326ba4fd983d011","IPY_MODEL_ee9d7315390f40f2b16d934c5f1035ec","IPY_MODEL_72242088179e4c07bf4378704755ee94"],"layout":"IPY_MODEL_449391d978fd4090bcf819e2abcc32fb"}},"fe123d7d953e4df88326ba4fd983d011":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9814106b063846839db4ed36776e371c","placeholder":"​","style":"IPY_MODEL_33e431f3c9e546df8c47903dd88e3e21","value":"Downloading (…)cial_tokens_map.json: 100%"}},"ee9d7315390f40f2b16d934c5f1035ec":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_99f91a2aeb064d84af2d2ed638ed0581","max":150,"min":0,"orientation":"horizontal","style":"IPY_MODEL_75763e3e63274a939749a735da41661c","value":150}},"72242088179e4c07bf4378704755ee94":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ad40377798734cd5922b5dfb6f0946c4","placeholder":"​","style":"IPY_MODEL_ecf209dbd61c4385b6272ca8d5febc28","value":" 150/150 [00:00&lt;00:00, 9.49kB/s]"}},"449391d978fd4090bcf819e2abcc32fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9814106b063846839db4ed36776e371c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"33e431f3c9e546df8c47903dd88e3e21":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"99f91a2aeb064d84af2d2ed638ed0581":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75763e3e63274a939749a735da41661c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ad40377798734cd5922b5dfb6f0946c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ecf209dbd61c4385b6272ca8d5febc28":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"73145279f67c4cdb90942dcf6cc5d4fe":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_45001572193b46c0b5a0df644bdc8e32","IPY_MODEL_80d71a6d85cc446c9f996c6abe6340bc","IPY_MODEL_05a75db33f3846f0b102c87fc7a630c8"],"layout":"IPY_MODEL_64aad1b7f58849eb9e2aeae48863b8ea"}},"45001572193b46c0b5a0df644bdc8e32":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9daf7f9869b49758cd479a914f98a44","placeholder":"​","style":"IPY_MODEL_687be3d975634437afd4aa59aa8045dc","value":"Downloading (…)okenizer_config.json: 100%"}},"80d71a6d85cc446c9f996c6abe6340bc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f54ccb746f744468ae8e7ee6a0170c4c","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_36fe258b872747e7b8c068dc7e99face","value":25}},"05a75db33f3846f0b102c87fc7a630c8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9dfc05a1afc845f5aaa58aebe0eddf10","placeholder":"​","style":"IPY_MODEL_9d2ee4ffc0b947a79717b0af84dee8d9","value":" 25.0/25.0 [00:00&lt;00:00, 1.59kB/s]"}},"64aad1b7f58849eb9e2aeae48863b8ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9daf7f9869b49758cd479a914f98a44":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"687be3d975634437afd4aa59aa8045dc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f54ccb746f744468ae8e7ee6a0170c4c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36fe258b872747e7b8c068dc7e99face":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9dfc05a1afc845f5aaa58aebe0eddf10":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d2ee4ffc0b947a79717b0af84dee8d9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a13969cb3c1f4eb888f9d438767c2471":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5723d901e2434772b402c45621448052","IPY_MODEL_17856e7f952640d0bfd6170b25e2699e","IPY_MODEL_24095d2602654379b1a496a64ffd84a6"],"layout":"IPY_MODEL_fdbe5c61a5044fad9a7fc0f592febefa"}},"5723d901e2434772b402c45621448052":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0642c8778fd746d8a63d6de76372bda5","placeholder":"​","style":"IPY_MODEL_c4d55917fd4749ac99889ebe27f312fc","value":"Downloading (…)lve/main/config.json: 100%"}},"17856e7f952640d0bfd6170b25e2699e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9ae85f65f5354a8db6b58f7df505a5db","max":498,"min":0,"orientation":"horizontal","style":"IPY_MODEL_424f130407b541428fa019b450b8b049","value":498}},"24095d2602654379b1a496a64ffd84a6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f9777ef4f344f3ea0b298af6f6eb2db","placeholder":"​","style":"IPY_MODEL_1dae1b1989614ca2be112e0f0cb786a5","value":" 498/498 [00:00&lt;00:00, 36.0kB/s]"}},"fdbe5c61a5044fad9a7fc0f592febefa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0642c8778fd746d8a63d6de76372bda5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4d55917fd4749ac99889ebe27f312fc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9ae85f65f5354a8db6b58f7df505a5db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"424f130407b541428fa019b450b8b049":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7f9777ef4f344f3ea0b298af6f6eb2db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1dae1b1989614ca2be112e0f0cb786a5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7561661529c14d33b2279b2f4c0d05d9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fe6f2c439e9d477695040db56a44ed8a","IPY_MODEL_d12190b672c94f6dacf44474c9ce7b9a","IPY_MODEL_9690f5db36454fd096700182b8af2a5e"],"layout":"IPY_MODEL_81a70830671042a59c1a3555facd612d"}},"fe6f2c439e9d477695040db56a44ed8a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3be684b9d14a479094d9a4ae5e24a1ce","placeholder":"​","style":"IPY_MODEL_dc8ef5dfa1134905897b9fc88a040503","value":"Downloading pytorch_model.bin: 100%"}},"d12190b672c94f6dacf44474c9ce7b9a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_988a9ec4b91a43dd9f0e35f3a6ed861a","max":498627950,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a27441962f5a4468926d65fd1f77a8d4","value":498627950}},"9690f5db36454fd096700182b8af2a5e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_152e8f6801624d2e95fc2ed18785ac05","placeholder":"​","style":"IPY_MODEL_6980c3464f9448dca5b65f55d02b8f6b","value":" 499M/499M [00:01&lt;00:00, 335MB/s]"}},"81a70830671042a59c1a3555facd612d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3be684b9d14a479094d9a4ae5e24a1ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc8ef5dfa1134905897b9fc88a040503":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"988a9ec4b91a43dd9f0e35f3a6ed861a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a27441962f5a4468926d65fd1f77a8d4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"152e8f6801624d2e95fc2ed18785ac05":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6980c3464f9448dca5b65f55d02b8f6b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}